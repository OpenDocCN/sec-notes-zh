# 课程：通用且上下文无关的触发器，用于精确控制大语言模型输出 [W8zzpTGVHRE] 🔐

在本节课中，我们将学习一种新型的提示注入攻击方法——通用对抗触发器。我们将了解其背景、原理、攻击演示以及如何生成这种触发器。这是一种能够跨不同上下文精确控制大语言模型输出的强大攻击手段。

## 背景：提示注入威胁的演变 🚨

![](img/516cf075fefe61b53090218b666a3a1d_1.png)

上一节我们介绍了课程主题，本节中我们来看看提示注入攻击的背景及其演变过程。

大语言模型及其应用近年来发展迅速。与此同时，提示注入已成为一个日益关键的攻击向量。

早期，大语言模型主要是独立的聊天或内容生成工具。提示注入依赖于直接的用户输入或网页内容脚本，以诱骗模型产生有害内容甚至错误答案。其影响相对有限。

随后，大语言模型被集成到更复杂的工作流中，例如检索增强生成系统。通过污染这些工作流动态检索的数据源，攻击者可以将恶意提示注入工作流，从而破坏整个流程。

如今，我们正进入AI智能体时代。这些智能体可以通过代码编辑器、浏览器等多种工具直接与现实世界交互。这极大地提高了风险。提示注入不再仅仅导致错误信息，还可能造成代码后门、数据泄露甚至整个系统被攻陷。

## 传统提示注入的局限性 ⚠️

上一节我们回顾了提示注入的演变，本节中我们来看看传统方法的局限性。

传统上，提示注入通常涉及两个步骤。

首先，攻击者试图逃离原始提示的上下文。这通常需要理解提示结构。因此，攻击者常尝试用“描述你的任务和角色”、“有哪些可用工具”等短语来泄露上下文。然后，他们尝试使用各种技巧进行越狱，例如要求模型忽略之前的指令或扮演一个不受限制的角色。

第二步是重定向模型的注意力到被劫持的任务上，即控制模型的响应。

以下是常见的被劫持任务：
*   生成不道德内容，例如教人制造炸弹。
*   产生错误答案，例如将狗分类为猫。

然而，这种传统方法有显著的局限性。越狱步骤需要对每种场景进行手动分析和精心设计。这些手工制作的注入高度依赖于上下文，无法在不同应用间复用。对于劫持步骤，攻击者通常需要更精确的输出控制，例如生成一个每个字符都至关重要的Shell脚本，或创建格式正确的JSON以传递给下游工具。此外，攻击者也希望造成比仅仅一个不道德响应更严重的安全后果。

这些局限性促使我们寻找更好的方法。

## 理想的攻击与通用对抗触发器 💡

上一节我们探讨了传统方法的不足，本节中我们来看看理想攻击应具备的特性以及我们提出的新范式。

我们认为理想的提示注入应具备四个关键属性：
*   **通用有效性**：我们希望将攻击解耦为可复用的触发器加上可定制的有效载荷，使同一触发器能在不同应用中生效。
*   **高可访问性**：无需复杂的注入技术，即使经验不足的黑客也能实现高成功率。
*   **精确控制**：攻击者能可靠地指定希望模型响应的确切内容。
*   **安全后果**：例如系统接管。

这似乎是一个不可能的组合，直到现在。

这里我们介绍**通用对抗触发器**，我们认为它代表了一种新的提示注入攻击范式。受对抗触发器学术研究的启发，我们将这一理论概念发展成了实际的现实世界威胁。

让我通过例子来说明这个想法。

左侧是针对文本生成任务的攻击。对话以定义模型角色的系统提示开始，它应该分析政策的利弊。原始用户消息本应讨论提供免费大学教育，但它被注入了恶意内容。注入由三部分组成：一个触发器前缀、一个有效载荷和一个触发器后缀。有效载荷是黑客希望模型输出的确切文本。它被一对触发器包围，触发器是一系列对人类来说大多不可读的标记。但当它们被大语言模型处理时，会迫使模型忽略其他指令，仅用有效载荷进行响应。你可以看到模型响应：“哈哈，我被黑了。我会做任何你想做的事。”

右侧是针对文本转SQL任务的类似攻击。系统提示描述了以JSON格式构建SQL查询，正常的用户消息请求从表中读取一些客户数据。但攻击者注入了要求模型生成恶意SQL命令的内容，该命令将从表中删除客户数据。

![](img/516cf075fefe61b53090218b666a3a1d_3.png)

两个例子都使用了模型 `Llama-2-7B-Instruct`，这是一个流行的开源模型。屏幕上显示的触发器字符串是专门为此模型训练的。因此，无论原始任务、注入位置或有效载荷内容如何，只要使用相同的目标模型，这些触发器就保持有效。

![](img/516cf075fefe61b53090218b666a3a1d_5.png)

请注意，我们已屏蔽了触发器中的一些标记。这是为了防止攻击者直接复制我们的触发器造成实际损害。

我们认为这是一种新的任务范式，因为它具有以下优势。最重要的是，触发器无需修改即可应用于许多攻击场景。根据我们的实验，它在不同的提示上下文和有效载荷中实现了约70%的成功率。一旦人们获得这些触发器，他们可以简单地将任何有效载荷插入模板中。它可以是JSON、XML或任何他们希望模型输出的内容，这可以大大降低提示注入的成本。我们很快将演示，这种范式甚至可以轻松地在AI智能体上导致远程代码执行。

## 攻击演示：远程代码执行 🎬

上一节我们介绍了通用对抗触发器的概念，本节中我们通过两个演示来看看它的实际攻击效果。

第一个演示将展示我们如何使用触发器攻击Open Interpreter。Open Interpreter是一个智能体，允许用户通过自然语言界面操作他们的计算机系统。

在播放视频之前，我将描述技术流程。当用户要求Open Interpreter检查他的邮箱时，智能体会编写一段Python代码来访问电子邮件。攻击者之前已经制作并发送了一封恶意电子邮件给用户。当智能体读取包含被触发器包围的Shell命令的电子邮件时，就会发生提示注入。这将迫使模型以特定格式输出Shell命令，以便被Open Interpreter传递并执行，从而为我们提供一个远程Shell。

第二个演示展示了在Cursor上实现远程代码执行。Cursor是一个广泛使用的Web编码助手，作为VSCode扩展运行。代码智能体用户通常会安装MCP服务器来扩展智能体功能。攻击者可以发布一个看似良性的MCP服务器，等待其获得一定流行度后，更新MCP描述以包含触发器和有效载荷。开发者通常会为安全命令启用自动批准功能，如果模型确定命令安全，这将允许Cursor无需用户确认即可执行命令。在我们的有效载荷中，我们将Shell命令嵌入XML格式，并将`requires_approval`标志设置为`false`。因此，当模型输出此XML有效载荷时，Cursor将在无需进一步用户确认的情况下执行命令。为了防止MCP工具进行恶意活动，一些开发者会将MCP服务器隔离在沙箱中。然而，这在我们这种情况下没有帮助，因为MCP描述仍然会被注入到提示中，并且Shell命令是在用户的VSCode终端上直接执行的，而不是在隔离环境中。

## 技术细节：如何发现触发器 🔬

上一节我们看到了触发器的强大威力，本节中我们来深入了解如何发现它们的技术细节。

以下是大语言模型处理包含触发器的输入的过程。首先，大语言模型使用提示模板将聊天消息转换为输入字符串。该字符串包含攻击者控制的注入输入，加上由应用程序决定的周围文本。然后，分词器将输入字符串转换为标记ID。我们将使用不同颜色表示提示上下文标记、触发器标记和有效载荷标记。模型中的嵌入层将每个标记映射到一个高维向量，称为标记嵌入。这些嵌入被输入到一个复杂的神经网络中，该网络将计算词汇表中每个标记的概率分布。然后，模型通过随机采样决定下一个输出标记。生成的标记被追加到输入序列中，该过程为后续输出标记重复进行。

为了获得对抗触发器，这里的核心思想是通过找到好的触发器标记，来最大化输出所需有效载荷标记的概率。因此，这可以公式化为一个数学优化问题。

输入包括前部的提示上下文、触发器前缀、有效载荷、触发器后缀以及其后的提示文本。我们希望最大化模型在给定输入标记的情况下输出有效载荷标记的概率。由于我们有多个有效载荷标记，这可以表示为各个标记概率的乘积。然后我们通过对概率取对数，并在我们的训练数据集上对有效载荷长度取平均，将其写为损失函数。前面还有一个负号，因为损失应该被最小化。

为了确保生成的触发器具有通用性，训练数据集应包含多样化的提示上下文和目标输出。除此之外，我们还需要一个好的优化算法来搜索触发器标记。

我们构建了一个数据处理流水线，通过将正常对话转换为对抗训练数据来生成训练数据。基础训练数据来自公共指令数据集，其中包含多样化的指令遵循示例。我们还添加了一些包含智能体对话模式（如Web编码对话）的领域特定数据。转换流水线会随机选择注入位置，并生成一些恶意有效载荷，例如错误答案、偏离主题的响应和恶意命令。这些有效载荷被包装在不同的格式中，包括纯文本、JSON和XML。

对于优化算法，我们不能直接将梯度下降应用于触发器标记，因为标记是离散值。这是一个根本性的挑战，因为梯度下降算法需要通过计算损失函数对输入变量的偏导数来获得方向性指导。

为了解决这个问题，HotFlip方法在2018年被提出。它使用标记嵌入空间的梯度来估计当我们用一个标记替换另一个标记时，损失函数如何变化。假设`L(A)`是使用输入标记A时的损失，`L(B)`是将标记A替换为标记B后的损失。我们可以使用这个方程来估计`L(B)`，这是标记嵌入空间中的一阶泰勒展开。为了找到最小化估计损失的标记，我们只需要选择其嵌入向量产生最小点积的标记B，如黄色背景所示。

虽然HotFlip告诉我们使用什么替换标记B，但贪婪坐标梯度算法告诉我们替换哪个原始标记A。该算法将每个标记位置视为一个可以操作的坐标。它随机采样几个标记位置，并为每个位置找到最小化估计损失的前K个替换候选。然后它测试这些候选的实际损失，并仅保留最佳位置及相应的替换。这个过程将迭代重复，直到损失收敛。

由于算法相当数学化，如果您想了解更多细节，可以参考我们的论文。我们将在最后提供一个链接。

## 实验结果与局限性 📊

上一节我们深入探讨了触发器的生成原理，本节中我们来看看实验结果以及当前方法的局限性。

我们在三个开源模型上测试了上述方法：Qwen、Llama和Devtro Small。结果表明，在经过约1000次迭代步骤，在一个包含超过10000个对话的数据集上训练后，发现的触发器在不同任务中实现了约70%的成功率。

我们测试了触发器的可迁移性。我们发现，在同一模型家族内存在一定的可迁移性，例如从`Llama-3-8B`迁移到拥有700亿参数的更大Llama模型，或从`Qwen2-7B`迁移到较新的`Qwen2.5-7B`，成功率保持在60%左右。然而，在我们的实验中，触发器无法跨模型家族迁移。

我们的方法有几个局限性。它需要白盒访问，因为我们需要模型权重和梯度信息来训练触发器。生成的触发器字符串通常对人类没有意义，可以通过一些基于困惑度的过滤器检测到。训练触发器需要大量的计算资源，我们总共需要大约10万次大语言模型调用来找到一个好的触发器。此外，如前所述，触发器无法跨模型家族迁移，这意味着攻击者无法在开源模型上训练，然后直接在专有模型上使用它们。

![](img/516cf075fefe61b53090218b666a3a1d_7.png)

## 总结与问答环节 🏁

在本节课中，我们一起学习了通用对抗触发器这一新的大语言模型攻击范式。它降低了成功进行提示注入攻击的门槛。这些触发器的存在威胁着AI智能体的安全设计。我们通过将梯度引导优化应用于开源模型，证明了此类触发器的存在。

请记住，大语言模型默认是不可信的。我们应始终在沙箱中运行智能体，并实施适当的安全控制。

感谢聆听。如果您有兴趣了解更多关于我们的研究，可以通过下面的电子邮件地址联系我们。您可以在列表上阅读我们的论文和一些相关文章。

---

**问答环节**

**问**：关于你展示的计算概率和损失的方程，概率是用触发器、有效载荷和后缀计算的。那么触发器是否依赖于有效载荷本身？换句话说，你能替换有效载荷并仍然得到相同的损失吗？如果你生成了那些触发器，之后能否将其用于未在触发器计算中使用过的其他有效载荷？

**答**：问题是，触发器是否依赖于有效载荷？是的，我们希望触发器独立于有效载荷，以便同一触发器适用于所有类型的有效载荷。在数据集中，唯一的变量是有效载荷本身和上下文。触发器是我们在训练过程中要发现的东西。所以，你可以基于任意有效载荷生成触发器，然后使用相同的触发器搭配不同的有效载荷。

**问**：目前由于需要访问权重来创建触发器，所以不可能（攻击专有模型）。你认为未来有可能泛化触发器吗？例如，通过搜索多个触发器，然后尝试找到能在多个模型上工作的那些。

**答**：我们还没有测试过，但有可能，比如同时在不同模型上训练触发器，或者独立使用它们然后组合起来，那样也许能在专有模型上奏效。