# 课程 01：安全港湾还是危险水域？揭示 PyTorch 中 TorchScript 引擎的隐藏风险 🚨

在本节课中，我们将学习 PyTorch 框架中一个重要的安全特性 `with_onnx`，并深入探讨其背后的 TorchScript 引擎如何被绕过，从而引发严重的安全漏洞。我们将从背景知识开始，逐步分析漏洞的成因、利用方式及其对整个 AI 生态系统的影响。

---

## 背景知识介绍

PyTorch 是一个基于 Torch 机器学习库的框架，由 Facebook 开发，现已开源。它易于使用，功能强大，广泛应用于深度学习研究、自然语言处理和计算机视觉领域。

在机器学习领域，存在许多流行的框架，如 TensorFlow 和 Keras。然而，根据官方数据，PyTorch 无疑是最受欢迎的机器学习框架之一。因此，在这样一个著名的框架中发现漏洞将是一件非常引人注目的事情。

---

## 故事的开始：从 Pickle 到 `with_onnx`

早期，PyTorch 为了方便，使用 Python 的 `pickle` 模块来加载和保存模型。然而，`pickle` 是 Python 中最著名的安全问题之一。事实上，官方 Python 文档警告称，加载不受信任的数据时使用 `pickle` 是不安全的。

2021年，有人在 GitHub 上提出了这个问题，并呼吁寻找解决方案。最终，在 2022年，PyTorch 发布了一个重要更新，引入了 `with_onnx` 参数。

让我们简要了解一下 `with_onnx` 的实现。当 `with_onnx` 未启用时（这是默认设置），PyTorch 会使用 `pickle` 来保存和加载模块，这会引入安全风险。然而，当我们设置 `with_onnx=True` 参数时，它将使用一种称为 `with_onnx_pickle` 的受限 `pickle` 来加载模型。

以下是一个示例，展示了默认情况下如何利用 `pickle` 执行命令：

```python
import torch
import pickle

class EvilPickle:
    def __reduce__(self):
        import os
        return (os.system, ('whoami', ))

evil_model = EvilPickle()
torch.save(evil_model, 'evil_model.pt')
loaded_model = torch.load('evil_model.pt', with_onnx=False)  # 这将执行 `whoami` 命令
```

然而，当使用 `with_onnx=True` 时，会引发异常，似乎有效地缓解了漏洞。在官方 PyTorch 文档中，他们也认为 `with_onnx=True` 是安全的，同时警告永远不要使用 `with_onnx=False` 加载不受信任的检查点。

此外，当在其他基于 PyTorch 的组件中发现类似的安全问题时，他们通常通过使用 `with_onnx=True` 来修复。例如，最初 VOM 使用 `torch.load` 时没有设置 `with_onnx=True`，从而引入了安全风险，然后他们通过添加 `with_onnx=True` 参数来修补这个漏洞。

不仅限于 VOM，每当发现软件使用 `torch.load` 而没有设置 `with_onnx=True` 时，修复方法总是相同的：只是添加这个参数。但这里出现了一个问题：我们能盲目信任 `with_onnx` 的安全性吗？

---

## 深入分析：`with_onnx` 如何工作？

之前我们指出，`with_onnx` 类似于一个受限的 `pickle`。在分析 `with_onnx` 的实现之前，我们需要了解 `pickle` 是如何工作的。

`pickle` 就像一个解释器，它按顺序读取并解释每段代码，执行相应的操作。例如，我们首先遇到 `GLOBAL` 操作码，它将执行 `load_global` 函数，加载目标函数（如 `os.system`）并将其推入栈中。

然后，我们需要构造参数。首先，执行 `UNICODE` 操作码，它将推入一个字符串（例如 `'whoami'`）。由于命令必须是元组格式，我们需要用 `TUPLE1` 操作码包装这个字符串，执行 `load_tuple` 函数。最后，使用 `REDUCE` 操作码，从栈中获取函数和参数，并执行该函数。

那么，`with_onnx` 如何解决这个问题？通过上面的例子，我们可以大致感觉到，`pickle` 反序列化中最危险的部分是 `load_global` 和 `load_reduce`。`with_onnx_pickle` 只是对这些危险的操作码添加了一些限制。

查看代码，可以看到 `load_global` 函数添加了白名单和黑名单检查以防止安全问题。同样，`load_reduce` 函数也以类似的方式受到限制，它也有一个白名单。

总的来说，`with_onnx` 是使用白名单和黑名单机制实现的。那么，我们如何绕过它呢？

一个常见的方法是检查白名单，看看是否有任何潜在的危险代码。我做了同样的尝试，但最终没有找到任何有用的东西。

我们常常受限于固定的思维方式。例如，大多数人可能只看到明显的白名单和黑名单机制，然后就停在那里。然而，实际上，使用 `with_onnx=True` 的加载过程相当复杂。那么，在这个加载过程的其他地方是否可能存在一些问题呢？

我尝试对整个工作流程进行复杂分析，以确保没有遗漏任何东西。我开始逐步调试代码，一行一行地执行。在一次练习中，我发现了奇怪的东西：`torch.jit.load`。这是什么？我对此一无所知，于是在谷歌上搜索了一下，发现 `torch.jit.load` 用于加载 TorchScript 格式的模块。

现在你可以理解为什么我们命名这个主题为“揭示 TorchScript 引擎的隐藏风险”了。接下来，我们将分析和利用 TorchScript 引擎。

---

## TorchScript 基础

TorchScript 用于在没有 Python 解释器的环境中部署模型，例如 C++ 和移动设备。它可以通过脚本或追踪从 Python 代码编译而来，并保存为 `.pt` 文件，该文件可以在 C++ 和移动设备中加载和运行。保存和加载过程涉及序列化。我将解释 Python 代码如何编译为 TorchScript、序列化和反序列化过程、`.pt` 文件的结构，以及最终如何执行。

首先，我们可以使用 `@torch.jit.script` 装饰器将一个 Python 函数编译为 TorchScript。

编译步骤如下：
1.  将 Python 源代码解析为抽象语法树。
2.  将 Python AST 转换为 JIT AST。
3.  将 JIT AST 转换为中间表示图。

IR 图最初包含许多不必要的节点，通过优化使其更加简洁。

让我们深入了解编译过程。在开始之前，我需要展示在这个上下文中什么是函数和模型。你可以看到一个是函数，另一个是类。这两者的脚本化过程有些不同，因此我们将分别介绍。

首先看函数的情况。编译过程在 `script_impl` 函数中处理。有两个主要函数：`get_jit_def` 和 `jit_script_compile`。`get_jit_def` 函数执行两个主要任务：首先，它使用 AST 库将 Python 代码转换为 Python AST；其次，它调用 `build_def` 将该 Python AST 转换为 JIT AST。`jit_script_compile` 是一个从 C++ 绑定到 Python 的函数，它在 C++ 层进一步将 JIT AST 转换为 IR。它最终调用 `to_ir` 函数，完成从 JIT AST 到 IR 的转换。此外，`to_ir` 还会调用一些优化函数来简化 IR 图。

现在，函数的编译过程就完成了。对这个过程有一个高层次的了解就足够了。如果你对完整的源代码细节感兴趣，可以使用我提供的调用栈来浏览源代码。

然后，让我们看看模型的情况。整体过程与函数类似，只是在开始时多了一个步骤：从模型中检索所有函数。它需要检索函数并逐个编译它们。`stubs_fn` 函数负责处理这个。`stubs_fn` 是 `init_methods_to_compile` 在这个上下文中的别名。让我们看看 `init_methods_to_compile` 函数做了什么。

在 `init_methods_to_compile` 函数中，首先使用 `dir()` 获取模型的所有属性和方法，经过过滤后，保存需要导出的方法，从而收集模型中所有需要编译的函数。`make_stubs_from_methods` 使用 `get_jit_def` 函数将收集到的函数转换为 JIT AST，这与函数编译过程中使用的函数相同。回到 `create_script_model`，`method_stubs` 变量存储我们刚刚转换的 JIT AST。`create_methods_and_properties_from_stubs` 函数以 `method_stubs` 作为输入，负责将其转换为 IR。我们可以看到，这个调用栈与函数编译的调用栈相似，两者最终都完成了向 IR 的转换。现在，模型的编译过程就完成了。

总的来说，函数和模型的一些关键步骤是相同的。两者都使用 `get_jit_def` 将 Python 转换为 JIT AST，然后通过 `to_ir` 转换为 IR。

序列化功能可以保存模型并将其部署到其他环境。使用 `torch.save` 将模型保存为 `.pt` 文件，使用 `torch.load` 将 `.pt` 文件加载为模型。

这里的 Python 代码是从 IR 转换而来的。你会注意到它与原始代码略有不同。

让我们看看脚本函数和脚本模型是如何序列化的。与脚本模型相比，脚本函数多了一个步骤：它需要先转换为脚本模型。之后，两者都调用 `save` 进行序列化。

在序列化过程中，一些信息以 pickle 格式存储在 `data.pkl` 和 `constant.pkl` 中。此外，它还将从 IR 转换的 Python 代码和调试信息保存到 `code` 目录中。

总结保存过程：`torch.jit.script` 函数首先转换为 Torch 模型。剩余的过程是相同的：首先，与模型对应的 IR 以 pickle 格式序列化为 `data.pkl`；其次，通过 `pretty_print` 获取代码和调试信息，并写入 `code` 目录；第三，将张量常量保存到 `constant.pkl`。现在，序列化完成。

让我们看看 `.pt` 文件的细节。`.pt` 文件是使用 zip 压缩的压缩文件。解压缩后的文件树如上所示。我们可以使用 `pickletools` 反汇编它，上面是 `data.pkl` 的反汇编输出。这段 pickle 代码的含义是创建一个名为 `torch.jit._pickle.Pickleable` 的对象，并将其 `training` 属性设置为 `True`。而 `torch.jit._pickle.Pickleable` 对象就是脚本模型。因此，在解释 `data.pkl` 时，模型对象被创建。`constant.pkl` 包含可以创建带有张量的 IR 对象的 pickle 代码。`torch.py` 文件包含 Python 代码字符串，它与脚本模型中 `code` 属性的内容相同，因为它们是以相同的方式获取的。

![](img/e832b22dff82c89e43512f2a11d340bc_1.png)

好的，现在我们理解了序列化文件中保存的三个最重要部分：`data.pkl`、`constant.pkl` 和 `code` 目录中的源代码。让我们看看这些部分如何恢复为脚本模型。

在反序列化函数中，序列化文件的三个主要部分被转换为脚本模型。让我们看看函数实现。`deserialize` 会调用 `read_zip` 来读取 `constant.pkl` 和 `data.pkl`，然后得到一个 C++ 中的模型，也就是 Python 中的模型。

在反序列化过程中，首先，`unpickle_constants` 小心地获取张量常量，然后保存在常量表中。其次，在反序列化 `data.pkl` 的过程中，从 `code` 目录读取相关源代码，并根据需要进行解析。同时，相关常量也从常量表中读取。代码解析过程包括将 Python 代码转换为自定义令牌，然后将令牌转换为 JIT AST，最后转换为 IR。这个过程与之前 Python 到 TorchScript 的编译过程有显著不同。

总结加载过程：首先，通过 `deserialize` 运行主要逻辑；其次，调用 `read_zip` 读取 `constant.pkl`，通过 `unpickle` 将常量转换为 IR 值，并将它们保存到常量表中；第三，调用 `read_zip` 读取 `data.pkl`，在反序列化 `data.pkl` 期间，源导入器读取 `code` 文件和常量表，通过 `parse_type`、`find_name_type` 和 `import_name_type` 恢复 IR 图。现在，反序列化完成。

![](img/e832b22dff82c89e43512f2a11d340bc_3.png)

让我们看看 TorchScript 是如何运行的。IR 图由许多节点组成，每个节点包含输入、输出和操作码三部分。操作码是执行的关键。执行 TorchScript 的过程是遍历 IR 并将其转换为指令，然后解释这些指令。每个指令都有一个操作码，这里有一个查找表，通过操作码找到对应的处理逻辑，然后执行它。

现在，我们对 TorchScript 的工作流程有了基本的了解。在阅读代码时，我发现了一些有趣的东西。有一个操作码名为 `OP`，它可以在操作符表上进行调用。操作符表包含许多函数，对应于操作符。我好奇它们是什么操作符。

一些内置函数将自己注册为操作符，需要操作指令来调用相应的函数。`RegisterOperators` 类管理这些操作符，并通过 `register_operator` 函数注册它们。我修改了 Python 代码，添加了一个函数来输出所有操作符。你可以看到这里有超过 2000 个操作符。所有这些操作符都安全吗？

在对这些操作符进行代码审计时，我们发现了两个有趣的操作符：`save` 和 `from_file`。这里我们可以看到这两个函数的相应实现。`save` 函数会将指定值作为模型保存到指定文件，这可能导致任意文件写入。但保存的文件包含许多脏数据，即模型信息。`from_file` 函数将读取指定的文件，并将文件内容转换为张量，然后返回该张量。我们可以从张量中获取文件内容。现在，我们知道这两个操作符存在漏洞。

但仍然有一个问题：如何从 TorchScript 中调用它们？要得到答案，我们需要进一步理解操作符的注册过程。

`_get_builtin_table` 函数将一些对象成员函数注册到内置操作符中。在这个过程中，它还会为成员函数添加前缀以进行重命名。最后，这些函数的地址及其修改后的名称将被放入内置表中。

在 TorchScript 编译过程中，当检查 Python 对象是否存在于内置表中时，会触发糖化操作。如果存在，则返回一个内置函数。最终，这个内置函数将通过 `emit_builtin_call` 插入到 IR 图中。

回到内置注册过程，我们现在理解 `save` 可能来自包含内置函数的对象之一。通过逐个测试每个对象的 `save` 函数过程，我们会发现 `save` 函数存在于 `torch` 对象中。`item` 和 `from_file` 也可以以相同的方式获得。

因此，我们只需要调用 `torch.save` 或 `torch.from_file` 就可以在 TorchScript 中获得任意文件读写能力。

现在，我们可以尝试写入一个文件来获得 RCE，但由于存在一些脏字符，只有特定的特殊文件才会起作用。`/etc/passwd` 文件因为权限不匹配而失败。我们的脚本使用 `torch.save`，它用 `0o600` 权限写入文件。但当前守护进程需要 `0o600` 权限。

这是一个漏洞验证视频。你可以看到我们获得了 RCE。

到目前为止，我们讨论的漏洞可能更类似于逻辑漏洞。另一方面，一些黑客可能更喜欢内存相关的漏洞。我们还发现了一些溢出问题。在这里，你可以看到当我们加载这个模型文件时，它触发了一个堆溢出。因此，我们可以观察到堆地址和库的加载地址被泄露。这是一个漏洞验证视频。但由于时间限制，我们不会在这里详细讨论它们。我们希望将来能分享它们。

好的，这就是这个 CVE 的全部细节。修复也相对简单：当启用 `with_onnx` 时，它阻止了 TorchScript 的使用。

---

## 漏洞影响与案例研究

那么这个漏洞对整个 AI 生态系统有什么影响呢？整个 AI 生态系统就像这座房子，而 PyTorch 是基础组件之一。如果 PyTorch 出现问题，就像我们移除了底层组件之一，整个结构的安全性就会受到损害。

我们在 GitHub 上对使用 `with_onnx=True` 的项目进行了简单搜索，可以看到很多项目都以这种方式使用它。这里我们选择两个知名项目作为例子来说明漏洞的影响：一个是 VOM，另一个是 Transformers。

VOM 是一个用于大语言模型推理的高性能库，针对速度和内存使用进行了优化。我选择它是因为它恰好有一个 CVE，该 CVE 指出它使用了没有设置 `with_onnx=True` 的 `torch.load` 函数，从而存在安全问题。他们通过添加 `with_onnx` 参数修补了这个漏洞，这似乎解决了问题。但由于 PyTorch 的问题，所有的缓解措施最终都无效了。因此，我们曾经认为的安全港湾实际上是危险水域。

同时，我注意到了一个非常有趣的现象。在 `requirements-cpu.txt` 中，你可以看到它硬编码了一个 PyTorch 版本，在 GPU 版本中也是如此。这意味着即使 PyTorch 发布了新的修补版本，VOM 的用户也无法及时更新，因为 VOM 已经硬编码了这个版本。因此，他们将长期暴露于这个漏洞之下。

首先，让我们设置我们的环境。由于 VOM 的最新版本已经修复了这个问题，我们需要安装一个旧版本，确切地说是 0.7.3。你可以看到，当我们安装 VOM 时，它会安装旧版本的 PyTorch。

这里我们将使用这个函数作为例子来演示漏洞的存在。起初，我没有想太多，只是想直接使用之前的漏洞利用代码，希望一次成功。你可以看到这里，当我们进入我们想要的逻辑时，它肯定会加载我们的模型。所以我们成功了。但事实上，失败了。为什么？让我们看看之前的漏洞利用代码，你可以看到在加载模型之后，我们需要调用这个模型才能触发前向逻辑，但在这里加载模型之后，你可以看到它没有调用模型，它只是获取 `at` 属性，然后删除这个模型。

这真的结束了吗？然而，在那个时候，我注意到了异常信息，它只是说模型对象没有名为 `at` 的属性。这个错误信息给了我一些启发。`at` 本质上是 Python 中的一个关键字，通常用于字典类型对象。你可以看到这里，实际上，`at` 是一个函数。那么，我们能推测出函数名吗？

于是我进行了第一次尝试。然而，当我们以这种方式编写代码时，我只是将函数名从 `forward` 改为 `items`，然后运行调用，但你可以看到我们仍然得到这个错误信息，为什么它仍然说我们没有 `items` 属性？但是，我尝试以这种方式编写，我们只是在我们的前向消息中调用 `items` 函数，你可以看到当我们运行代码时，我们成功地触发了逻辑。

![](img/e832b22dff82c89e43512f2a11d340bc_5.png)

这非常奇怪。原因是当检索模型函数时，它定义了该函数是否需要为函数导出，没有用 `@export` 装饰的函数，有一个延迟导出策略。这意味着只有在导出的函数内部调用它们时，