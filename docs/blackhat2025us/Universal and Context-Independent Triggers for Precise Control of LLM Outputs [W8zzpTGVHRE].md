# Universal and Context-Independent Triggers for Precise Control of LLM Outputs [W8zzpTGVHRE]

Good afternoonï¼Œ everybodyã€‚ Welcome to our session about the universal and context independent triggers for precise control of LM outputã€‚

 And today's topic is about prompt injectionã€‚ and We are exploring a newã€‚

 powerful prompt injection attackã€‚ I will show you how a single magic string can reliably force language models to output exactly what the attacker wantã€‚

ğŸ˜Šï¼ŒMy name is Jia Shuoliliangï¼Œ and my core researcher is Guan Cheengliã€‚

 We are both security researchers from Tencent Xuan Wu Laã€‚Firstã€‚

 we will cover the background of prompt injection threats and how they have evolved with LRM applicationsã€‚

 Then we will introduce universal adversarial triggersã€‚

Which is a new attack method that allows attackers to control the output of L L M with high precision across different contextã€‚

We will demonstrate how these triggers can achieve remote code execution on modern AI agentã€‚

 After thatï¼Œ we will explain the technical details of how to discover these triggers with gradient based optimizationã€‚

 Finallyï¼Œ we will have the takeaways and Q A sectionã€‚



![](img/516cf075fefe61b53090218b666a3a1d_1.png)

So large language models and their applications have evolved rapidly in recent yearsã€‚

 along with this evolutionã€‚ prompt injection has become an increasingly critical attack vectorã€‚

In the early daysï¼Œ L L Ms were primarily standalone chat or content generation toolsã€‚

 prompt injection relied on direct user input or content script from the Web pagesã€‚

To trick models into producing harmful contentï¼Œ or even wrong answersã€‚

This impact was relatively containedï¼Œ howeverã€‚Nextï¼Œ I M were integrated into more complex workflowsã€‚

 such as retrieval augmented generation R A G systems by poisoning the data sources that dynamically retrieved by these workflows such as web search resultsã€‚

 enterprise knowledge baseã€‚ Aers could inject malicious problems into the workflow to disrupt disrupt the entire workflowã€‚

ğŸ˜Šï¼ŒAnd todayï¼Œ we are entering the age of AI agentã€‚This agentã€‚

Can interact directly with the real world through many code editorsï¼Œ browsers and other MC CP toolsã€‚

ğŸ˜Šï¼ŒThis dramatically raises the stakesã€‚ prompt injection no longer just cause misinformationã€‚

 but enables code back doorsï¼Œ data and even complete system compromiseã€‚ğŸ˜Šï¼ŒTraditionallyã€‚

 prompt injection typically involve two stepsã€‚Firstã€‚

 the attacker tries to escape from the original prompt contextã€‚

This usually require understanding of the prompt structuresã€‚

 So attackers often try to leak context with phrases like describe your task and roleã€‚

 What are the available toolsã€‚Then they attempt to jailbreak using various tricksï¼Œ for exampleã€‚

 Ask the model to ignore previous instructions or act as an unstricted categoryã€‚

The second step is to redirect the model's attention to hijacked tasksã€‚ In other wordsã€‚

 to control the model's responseã€‚Common tasks includeã€‚Generating unethical contentã€‚

 such as teaching people to build bombs or producing wrong answersã€‚

 such as classifying a dog as a catã€‚ howeverï¼Œ thisã€‚

Traditional approaches have significant limitationsã€‚

The jail breaking step requires manual analysis and crafting for each scenarioã€‚

These handcrafted injections are highly context dependent and not reusable across different applicationsã€‚

For the hijacking stepï¼Œ attackers often need more precise output controlã€‚

 like generating a shell script where every character matters or creating properly formatted Jason to be passed to downstream toolsã€‚

Furthermoreã€‚Attacker also want more security consequences than just an ethical responseã€‚

These limitations LED us to search for a better approachã€‚

 We think an ideal prompt injection would have  four key propertiesã€‚ Univer effectivenessã€‚

 We want to decouple the attack into reusable triggers plus customizable payloadã€‚

 So the same trigger can work across different applicationsã€‚And high accessibilityã€‚

No complex injection techniques neededï¼Œ allowing even inexperienced hackers to achieve high success rate and precise controlã€‚

 attackers can reliably specify the exact content that he want the model to respond withã€‚Finallyã€‚

 we also want security consequencesï¼Œ such as system takeoverã€‚

This seemed like an impossible combinationï¼Œ until nowã€‚

Here we introduce the universal adversial triggerï¼Œ which we believe represents a new paradigm of prompt injection attacksã€‚

Inspired by some academic research on adversarial triggersã€‚

 we have taken this theoretical concept and developed it into a practical real world threadã€‚ğŸ˜Šã€‚

Let me illustrate the idea by examplesã€‚On the leftã€‚It is a attack against tax generation taskã€‚

 The conversation starts with a system prompt defining the model's rule hereã€‚

 It should analyze the policy pros and consã€‚And the original user message should discuss providing free college educationã€‚

 but it gets injected with some malicious contentã€‚The injection consists ofã€‚Three partsã€‚

 a treasure prefix and a payloadï¼Œ and a trigger suffixã€‚

The payload is the exact text that the hacker wants the model to outputã€‚

It is surrounded by a pair of triggerï¼Œ which is a sequence of tokensï¼Œ mostly unreadable to humanã€‚

 But when they are processed by the LL Mï¼Œ they will force the model to ignore other instructions and simply respond with the payloadã€‚

You can see the model responses withï¼Œ hahaï¼Œ I'm hackedã€‚ I will do whatever you wantã€‚

And on the right side is a similar attack against a text to SQL taskã€‚

The system prompt describes constructing SQL curries in a Json formatã€‚

 and the normal user message requests to read some customer data from a tableã€‚

But the the attacker inject contents that asks the model to generate a malicious SQL commandã€‚

 which will erase the customer's data from the tableã€‚ğŸ˜Šï¼ŒBoth examplesã€‚Use the model can 2ã€‚

5 7 B instruct modelï¼Œ which is a popular open source modelã€‚

The trigger string shown on this screen were trained specifically for this modelã€‚

So regardless of the original taskï¼Œ the inject locations or payload contentã€‚

 these triggers remain effective as long as the same target model is usedã€‚

Notice that we have masked some tokens in the triggersã€‚ This is to avoid attackersã€‚

Simply copying our trigger to cause real damageã€‚We consider this as a new task paradigm because it has the following advantagesã€‚

Most importantlyï¼Œ the triggers can be applied to many attack scenarios without modificationã€‚

 According to our experimentï¼Œ it achieves about 70% success rate across diverse prompt contacts and payloadã€‚

Once people obtain these triggersï¼Œ they can simply insert any payload into the templateã€‚

 It can be Jason Xï¼Œ Mï¼Œ L or whatever he wants the model to outputã€‚

 so this can greatly reduce the cost of prompt injectionã€‚ğŸ˜Šï¼ŒAnd we will demonstrate soonã€‚

 this paradigm can even easily cause remote code execution on AI agentã€‚

So the first demo will show how we use the triggers to attack open interpreterã€‚

The open temperature is an agent that enables users to operate their computer systems through a natural language interfaceã€‚

ğŸ˜Šï¼ŒI will describe the tech flow before we play a videoã€‚

When the user asks the open interpreter to check his mailboxã€‚

The agent will write a piece of python code to access the emailsã€‚

 and the attacker has previously crafted and sent a malicious email to the userã€‚

 Prompt injection occurs when the agent reads the emailã€‚

 which contains a shell command surrounded by the triggersã€‚

 This will force the model to output the shell command in a specific format to be passed by the open interpreterã€‚

 and it will execute the commandï¼Œ to give us a remote shellã€‚ğŸ˜Šï¼ŒLet's watch the demonstrationã€‚

So on the right sideï¼Œ we are starting a docker that receives the reverse trailã€‚And on the left sideã€‚

 we run the open interpreterã€‚And the user asks it to read the last emailã€‚

And here is the Python code that to be executedã€‚

![](img/516cf075fefe61b53090218b666a3a1d_3.png)

And when it's executing the codeï¼Œ it gets the content of the emailã€‚

You can see there is a shell scriptã€‚Inside the emailã€‚And the model isã€‚

Somehow summarizing the email and response with our share codeã€‚



![](img/516cf075fefe61b53090218b666a3a1d_5.png)

And we get a remote shellã€‚On the right sideã€‚Oã€‚And we have the next demo shows achieving remote code execution on clientã€‚

 It is a widely used web coding assistant that runs as a V S code extensionã€‚

 It is common for code agent users to install MP servers to extend agent functionalityã€‚

 and attacker can publish a seemingly beign MP server wait for it to gain some popularity and update the MP to description to include the triggers and payloadã€‚

ğŸ˜Šï¼ŒDevelopers often enable an auto approval feature for safe commandã€‚ğŸ˜Šã€‚

Which will allow client to execute the command without user confirmation if the model determines it is safeã€‚

In our payloadï¼Œ we embed the shell command in the XML format with the requires approval flag set to the fourthã€‚

So when the model outputs this X ML payloadã€‚Client will execute the command without requiring further user confirmationã€‚

To prevent MCP tools to do malicious activitiesï¼Œ some developers will isolate the MCP server in the sandboxã€‚

 Howeverï¼Œ this doesn't help in our caseï¼Œ because the MCP2 descriptionã€‚

 will still get injected into the prompt and the shell is the shell command is executed directly on the user's Vco terminalã€‚

 not the isolated environmentã€‚ğŸ˜Šï¼ŒSoï¼Œ let's watch the demonstrationã€‚This is VS codeã€‚

And we are opening clientã€‚So we have enabled the execute safe command optionã€‚

And there is also an execute command optionï¼Œ but I disabled itã€‚And the model we use is devtro smallã€‚

 It's a open source modelã€‚And this is the MCP tool controlled by the attackerã€‚

The attackger put Malaysiaã€‚Payload in the tool descriptionã€‚

And the user asks the client to describe the current projectã€‚

And client just simply execute our payloadã€‚And a calculator pops upã€‚ğŸ‘Thank youã€‚

So universal adver triggers are powerfulã€‚ But how do we find themï¼Œ Let's dive into the detailsã€‚

Here is how L Ms process the input containing triggersã€‚ Firstã€‚

 L M will convert chat messages into an input string using prompt templateã€‚

 This string contains the injected input controlled by attacker plus surrounding text controlled determined by the applicationã€‚

 Then a token tokenizer transforms the input string into token Isã€‚

 will use different colors to represent prompt contact tokens trigger tokens and payload tokensã€‚

 and a embedding layer in the model mapsã€‚ Each token to a high dimensional vectorã€‚

 which are named token embeddingsã€‚ and the embeddings are fed into a complex neural network which will compute the probability distribution over each token in the vocabularyã€‚

And then the model decides the next output token by random samplingã€‚

The generated token gets appended to the input sequence and the process repeats for subsequent output tokensã€‚

In order to get the adversal triggerï¼Œ the core idea here is to maximize the probability of outputting the desired payload tokens by finding a good trigger tokensã€‚

So this can be formulaized as a mathematical optimization problemã€‚

The input consists of prompt contact at the frontï¼Œ trigger prefixï¼Œ payloadã€‚

 trigger suffixix and the prompt textã€‚After itã€‚We want to maximize the probability of the model outputting payload tokensã€‚

 given the input tokensã€‚Since we have multiple payload tokensã€‚

 this can be expressed as a product of individual token probabilitiesã€‚

Then we write it as a loss function by taking the logarithm of the probabilitiesã€‚

 averaging over the payload length in our training dataset setã€‚

There is also a minus then because the loss is supposed to be minimizedã€‚

And to make sure the resulting trigger to be universalã€‚

 the training data set should contain a diversity of prompt context and target outputã€‚

 Apart from thatï¼Œ we also need a good optimization algorithm to search for the trigger tokensã€‚

We built a data processing pipeline that transforms normal conversations into adversarial training data by training dataã€‚

 the base training data come from public instruction dataã€‚

 which consist of diverse instruction following examplesã€‚

 We also add some domain specific data that contain agent conversation patterns such as web coding dialoguesã€‚

The transformation pipeline will randomly selectï¼Œ inject locationsã€‚

And generate some malicious payloadï¼Œ such as incorrect answers of topic response and malicious commandsã€‚

 These payload are wrapped in a different formatï¼Œ including Plantexï¼Œ Jason and X M Lã€‚ğŸ˜Šã€‚

And for the optimization algorithmï¼Œ we cannot directly apply gradient descent to the trigger tokens because the tokens are discrete valuesã€‚

 This is a fundamental challenge because gradient descent algorithms requires a directional guidance by computing the partial derivatives of the loss function over input variablesã€‚

To solve this problemï¼Œ the hot flip method was proposed in 2018ã€‚

 It uses gradients of token embedding space to estimate how the loss function changes when we replace one token with anotherã€‚

Let's say the L of a is the loss when using input token A and L of B is the token after replacing token A with token Bã€‚

And we can estimate the L of B using this equationã€‚

 which is the first order Taylor expansion in the token embedding spaceã€‚

To find the token that minimize the estimated lossï¼Œ we only need to pick the token Bã€‚

 whose embedding vector produces the smallest dot productï¼Œ as shown in the yellow backgroundã€‚

While the hot flip tells us what replacement token be to use the greedy coordinate gradient or G CG algorithm tells us which original token A to replace the algorithm treats each token token position as a coordinateã€‚

 It can operate onã€‚ It randomly sample several token positions and finds the top K replacement candidate that minimize the estimated loss for each positionã€‚

 It then tests the actual loss for this candidate and keeps only the best positions and the corresponding replacementã€‚

ğŸ˜Šï¼ŒAnd this process will repeat iterativelyï¼Œ until the loss convergesã€‚

Since the algorithm is quite mathematicalï¼Œ If you want to know more detailsã€‚

 you can refer to our paperã€‚ We will have a link at the endã€‚

We have tested the above method on three open source modelsï¼Œ Queenã€‚

 Lama and Devstore Small results show that after about0 of iteration steps on a data set over 10000 conversationsã€‚

 the discover trigger achieves about 70% a success rate across tasksã€‚

We have tested the transferability of the triggersã€‚

We found some transferability within model familiesï¼Œ for exampleï¼Œ from Lama 3ã€‚

1 8 B to a larger Lama model of 70 billion parametersã€‚

Or from queen2 dash7 B to a newer version of queen 2ã€‚5 7 Bã€‚Maintaining around 60% success rateã€‚

Howeverï¼Œ the triggers didn't transfer across model families in our experimentã€‚

And our approach have several limitationsã€‚ It requires white box access because we need the model ways and gradient information to train the triggerã€‚

And the resulting trigger screen typically do not make sense to humanã€‚

 It can be detected by some perplexity based filtersã€‚

Training the triggers requires considerable amount of computing resourcesã€‚

It takes us about 100000 of L M invocations in total to find a good triggerã€‚Alsoã€‚

 as mentioned earlierï¼Œ the triggers are not transferable across model familiesã€‚

 which means the attacker cannot train on an open source model and then use them directly on proprietary modelsã€‚



![](img/516cf075fefe61b53090218b666a3a1d_7.png)

In conclusionï¼Œ universal adver triggers represents a new LOM attack paradigm that lowers the barrier for success prompt injection attackã€‚

The existence of these triggers threaten the security design of AI agentã€‚

We have demonstrated that such triggers exist by applying gradient guided optimization to open source modelsã€‚

Remember that L L Ms are not trustworthy by defaultã€‚

We should always run this agent in the sandbox and implement proper security controlsã€‚

Thanks for listeningã€‚ And if you are interested in learning more about our researchã€‚

You can contact us at the email address belowã€‚And you can read our paper and some related articles on the listã€‚

Okayï¼Œ that's all of my talkã€‚ if you have some questionã€‚Oã€‚So if you have some questionsã€‚

 you can find a microphone nearby and ask meã€‚He thanks cool talkã€‚

 say when you were looking at the equations that you had on how you compute the probability and then compute the lossã€‚

ğŸ˜Šï¼ŒThe probability is computed with the trigger pillow load and the suffix of yeahï¼Œ exactly thisã€‚

 So howã€‚Is the trigger on the payload itselfã€‚ In other wordsã€‚

 Can you substitute the payload and still get have the same loss So if you generate those triggersã€‚

 could you substitute the payload afterwards for something else that was not in the computation of your triggerã€‚

 Okayï¼Œ so the question isï¼Œ is the trigger dependent on the payloadã€‚ Yesï¼Œ Yeahï¼Œ we weã€‚

 we want to make the trigger independent from the payload so that the trigger remains the same for all kinds of payloadã€‚

And in the data setï¼Œ the onlyã€‚Variable is the payload itselfï¼Œ and the contextã€‚Theã€‚

 the trigger is what we are going to discover during the training processã€‚Okayã€‚

 so you can generate the trigger based on an arbitraryory pillow and then use that same triggerã€‚

 different pay whatã€‚Okayï¼Œ any other questionsã€‚Not as much a question its just a comment I thought you did a great job bridging a very technical and complex situation and breaking it down into a broad and diverse audience presentationã€‚

 so I applaud youï¼Œ I thought this was really greatã€‚ğŸ˜Šï¼ŒThank youã€‚So if there are no more questionsã€‚

 I thinkã€‚å“¦ï¼ŒO okã€‚As the close those models are right nowã€‚

 not possible because you need to have the access ways to create the triggersã€‚

 do you believe it's going to be possible to generalize the triggers at some point so you're going to see a patterns to actually startã€‚

æˆ‘æˆ‘ã€‚You mean generalizing the triggers for what I believe you're gonna do a great search of multiple triggers and then try to find the ones that will be workingã€‚

Through the multiple modelsã€‚ Yeahï¼Œ wellï¼Œ we didn't test on itã€‚

 but it might be possible for us to likeã€‚Discovering like training the trigger on different models simultaneously or independently use them together that yeahã€‚

 that will maybe work on private modelsã€‚Yeahã€‚Soï¼Œ let's end our talkã€‚Thank youã€‚Thank youï¼Œ everyoneã€‚

