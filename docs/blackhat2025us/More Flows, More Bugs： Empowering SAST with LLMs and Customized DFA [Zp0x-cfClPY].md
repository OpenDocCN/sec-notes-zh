# More Flows, More Bugsï¼š Empowering SAST with LLMs and Customized DFA [Zp0x-cfClPY]

Good afternoonï¼Œ everyoneã€‚ Thank you for being hereã€‚ I'm Yuan from Tencent Security Reading Labã€‚

 And todayï¼Œ I will show you how we are enhancing static code analysisã€‚

 using large language models and customized data flow analysisã€‚ðŸ˜Šï¼ŒSo firstã€‚

 I will introduce what su isã€‚ Nextï¼Œ how we can use L Ms to automatically identify source and think functionsã€‚

Then our customized enhancement to data flow analysisã€‚ And finallyã€‚

 the results demonstrating the effect effectiveness of our approachã€‚What is SAã€‚

 Sas means static application security testing using SA toolsã€‚

 We can analyze source code without running the applicationã€‚ For exampleã€‚

 detecting SQL injection bugsã€‚From the picture on the rightã€‚

 we can see that sa is now an essential part of Devack opsï¼Œ automating security checksã€‚

Popular tools include code Q Lï¼Œ fortify and so onã€‚What is called Q Lã€‚

 It's now a product under Githubã€‚queries and libraries are open source under the M I T licenseã€‚

 including DA DFA partã€‚But the core engine is proprietaryã€‚ The workflow of code L has four stepsã€‚

Firstï¼Œ download the source code of the projectï¼Œ then create a database using code L command and then write a Q L query and run it against a databaseã€‚

 And finallyï¼Œ the query results show whether there are any bugsã€‚And on the wall of fame of code Co Lã€‚

 we can see all the bugs uncovered by code Co Lã€‚ It's not a complete listï¼Œ but so farã€‚

 it has found 418 bugsã€‚ and the numbers is still increasingã€‚ Also on Githubã€‚

 we can set up our projects to run code Co L scansã€‚

Either on a schedule or whenever this a pull requestã€‚ So does it mean no bugs anymoreã€‚

We tried using code L to detect recent high risk bugsã€‚ like the ones listed in this tableã€‚

 we found there were a lot of false negativesã€‚ We analyzed the root causes as listed in the last column and found two main reasonsã€‚

 First is in complete source and think coverage in built in publicationation rulesã€‚

 Second is disruptions in data flow due to insufficient support for certain language featuresã€‚

For the first problemï¼Œ I will introduce how to use L Ms to recognize source and think functionsã€‚

We have noticed that current methods rely heavily on manual workã€‚

 One approach is manual definition where developers have to manually review the source code to identify sources and thingsã€‚

 Another way is through community contributions like code Q L has default rules from the pictureã€‚

 We can see that key information includes libraryï¼Œ moduleï¼Œ function nameã€‚

But we have found both methods are pretty labor intensiveã€‚

 So is there a way to automate this processã€‚First problem isã€‚

 where should we look for sources and thingsã€‚We have found that source and think functions are actually API functions from third party frameworks from the picture on the leftã€‚

 we can see that S S I think actually is an API in resty framework and implement implementation code for this API functions can be found in open source frameworks So we can detect sources and things by scanning those frameworksã€‚

ðŸ˜Šï¼ŒThe second problem is how A Ms can helpã€‚ On the right is the workflow of our methodã€‚

 It has three AI agentsã€‚ We first use the discover agent to identify potential source and think functions in the frameworkã€‚

 Nextï¼Œ the judge agent applies expert rules to check the functions and remove results that don't align with expert experienceã€‚

ðŸ˜Šï¼ŒFinallyï¼Œ the validation agent wants queries to verify that the functions are used in real world projectsã€‚

ðŸ˜Šï¼ŒFirstï¼Œ we run a cross grain detection at the file levelã€‚

 feeding the source code file and per to the model in the perã€‚

 when many describe the characteristics of the functions we need to identifyã€‚

 which are related to the programming language and the bug typesï¼Œ for exampleï¼Œ for SSF bugsã€‚

 the sync functions are typically those sending H T T P requestsã€‚ And on top of thatã€‚

 we can set constraints on the format and style of the responseã€‚

 And you can try different pers to get the best resultsã€‚ And this is just a referenceã€‚ðŸ˜Šã€‚

And for the L M's resultï¼Œ will remove results with low confidence scoresã€‚

 and the threshold can be adjusted depending on the per and the model you useã€‚And in the endã€‚

 we'll get a list of source and sync functionsã€‚To remove the false positives from the previous stepã€‚

 we move on on to function level filteringã€‚ We check both the function names and function codeã€‚

 For exampleï¼Œ with is I Fã€‚ the sync is a function that sends an H TP requestã€‚

 while for sQL injectionï¼Œ the sink is executing an sQL statementã€‚

 And we have noticed that long thinking models tend to perform better at this stageã€‚ And againã€‚

 any functions that don't meet the criteria will be removedã€‚ In the endã€‚

 we collect all the source and sync functions present in each frameworkã€‚ðŸ˜Šã€‚

Besides having the L O M directly scanï¼Œ we also need to integrate expert experienceã€‚

 The experience comes from the community's longstanding practiceã€‚ For exampleã€‚

 the function should be publicly accessibleã€‚ The function should have return values that propagate tented dataã€‚

And the function shouldï¼Œ the return value of the function should not be of the boolean typeã€‚

 And there are plenty of these expert rulesã€‚And againã€‚

 they can be added or revised based on different bug types or 10 typesã€‚

 And we'll use the L M to apply these expert rules to check the function name and function body here and removing functions that don't match the expert rulesã€‚

The final step is confirming that the source and think we have noticed identified are actually used in real projectsã€‚

Firstï¼Œ we have to head to the framework project homep page and find the dependent pageã€‚

And this page lists most of the open source projects using that frameworkã€‚And nextã€‚

 download the source code of these projectsï¼Œ Soing them by star count from highest to lowestã€‚

And finallyï¼Œ run a Sa tool like code Co L with the new source Think rules applied to scan these projectsã€‚

 And based on the scan resultsï¼Œ verify if the newly added source and sync is actually used and any source or think that isn't used will be removedã€‚

ðŸ˜Šï¼ŒAnd for the second problemï¼Œ I' will introduce our enhancements to DFA data flow analysisã€‚

What is D FA in code Co L and other Sa tools on the left is an example of data flow analysis in code Co Lã€‚

 It works by extending data flow configure hereã€‚And implementing the source interface at line 10ã€‚

And is think interface at line 16ã€‚And then calling flowpath to perform a global10 analysis at line 27 hereã€‚

So on the right are the results From the resultsï¼Œ we can see a list of numbered path nodesã€‚

 And this list represents a complete data flow pathã€‚And each path node has two partsã€‚

 the node and the access pathã€‚So how does the whole process workã€‚

 And this diagram shows the workflow of DFAã€‚ Firstï¼Œ code Q L provides the confi user interfaceã€‚

 and you can call the flow path API just the last previous slideã€‚

 And then there are two key concepts node and access pathã€‚

Notode is calculated through forwardward flow and reverse flowã€‚

 Forward flow is the forward publication from the source pointã€‚

 While reverse flow goes the other wayï¼Œ tracing backward from the sync pointã€‚ And togetherã€‚

 they form a complete source to sync pathã€‚ðŸ˜Šï¼ŒAn access path is calculated in five stages from stage 1 to stage 5ã€‚

 Each stage adds further details to produce the final resultã€‚

We use forward flow to illustrate how node is calculatedã€‚

 On the right is the diagram of codeQ L source codeï¼Œ which consists of different steps such as jumpã€‚

 storeï¼Œ and details are in the tableã€‚First is the source stepã€‚

 which sets the starting point for data flowã€‚ Nextï¼Œ the local flow stepã€‚

 which handles intra procedural analysisã€‚And then the jump stepã€‚

 it provides the interface for custom propagation from one node to anotherã€‚ There are two interfacesã€‚

 A user level interface through its additional flow step and a system level interface through additional value stepã€‚

ðŸ˜Šï¼ŒAnd then comes the store and load stepã€‚ Store is writing a ten node into a fieldï¼Œ arrayã€‚

 collection or map content where load is reading a ten node from themã€‚

 And this is the core of access pathã€‚And finallyï¼Œ call inï¼Œ call out and go through stepsã€‚

 which deal with interprocedural analysisã€‚ðŸ˜Šï¼ŒCo in handles propagation from actual arguments to formal parametersã€‚

 Co out and go throughï¼Œ deal with propagating the return valuesã€‚

And the difference is whether the return value is propagated from external sources or parametersã€‚

And notice that post updated notes here represents notes that arelic implic updated as a result of a function callã€‚

So what is access pathï¼Œ It consists of a content list and a typeã€‚

 Con is basically a way to describe how data can be stored inside an objectã€‚

 and details are in the table on the left hereã€‚ So for a field typeã€‚

 its content is simply the field nameã€‚ for other typesã€‚

 it's more of generalized result as shown in the table hereã€‚

An access path keeps track of the content propagation relationships for the four types of nodesã€‚

 field arrayï¼Œ collection and map during store and load stepsã€‚

 And the table here shows examples of access pathã€‚Soï¼Œ for exampleï¼Œ in theï¼Œ this roleã€‚

 the content is name with the type stringã€‚Then how to calculate the access pathã€‚So on the right hereã€‚

 we write a sample code from the tableã€‚ You can see how the interry valueã€‚

Access path changes at each stage at the beginning at in stage 1ï¼Œ there's no A P at allã€‚

 And at stage 2ï¼Œ A P becomes a bullolean typeï¼Œ indicating whether a content per operation existsã€‚

 And at the lastï¼Œ we find out the complete access path is a content list here and a typeã€‚

And after an overview of D FAï¼Œ nowï¼Œ we introduce about some of the challenges it faces using Java as an exampleã€‚

So static analysis toolss tools include code Q L face challenges such as cross thread issuesã€‚

 reflection and value passingã€‚So here is an example of cost threat analysisã€‚

The parameters of the static static main method are the source pointã€‚

And the print line method parameters are the think pointã€‚So on the left here is the codeã€‚

And on the right is the intermediate results of the static analysisã€‚

And as we can see in the default analysisï¼Œ we can't find a path from source to syncã€‚

 It breaks at constructor call of roundable demo at line 14ã€‚So how do we connect itï¼Ÿ

 Remember the jump step we previously introducedï¼Œ we can use jump stepã€‚

To jump from the runable demo call to the instance parameter of the wrong methodã€‚

 namely the this parameterã€‚ And after the patchï¼Œ we get the complete pathã€‚

 So to summarize when the tent is propagated through the constructor over runable instanceã€‚

 our approach is to jump from the constructor call to the wrong methodã€‚

And this is the implementation using the additional value step interfaceã€‚

 And we complete the jump step hereã€‚But what if the10 isn't passed through the constructorã€‚

 such as through assignment statement in a function call in at line 8ï¼Œ How do we handle thisã€‚

 Our solution is that if the ten0 exists before the start interface is called like line 17ã€‚

 we take the color of the start method and jump to the instance parameter of the wrong methodã€‚

 So from here to hereã€‚So inï¼Œ in this caseï¼Œ jump from the line 18 to 10ã€‚ And after the patchã€‚

 we can get the complete pathã€‚And what if the 10 comes after the stock interfaceï¼Œ in this caseã€‚

 line 23ï¼Œ our approach is to find a node matches three conditionsã€‚ Firstã€‚

 this node is a post update noteã€‚ Secondï¼Œ this node is a roundable instanceã€‚ Thirdã€‚

 this node has a store operationã€‚ðŸ˜Šï¼ŒSo we jump from the that node to the instance parameter of the wrong methodã€‚

 So in this caseï¼Œ jump from line 8 to line 10ã€‚And after the patch you can see the complete flow is detectedã€‚

Nextï¼Œ we'll introduce the second challengeï¼Œ Java reflectionã€‚

And take the invoke method in this code as an exampleï¼Œ which is line 13ã€‚

 we are facing two difficulties hereã€‚ Firstï¼Œ how do we pinpoint the method that invoke actually correspond toã€‚

 So namelyï¼Œ what is the method at line 13ã€‚ Secondï¼Œ how do we fix the propagation relationships for interprocedural cause like call in and call throughã€‚

 So from the red lineï¼Œ we can see that the parameter propagation logic is different from the normal core processã€‚

For challenge1ï¼Œ there are two solutionsã€‚ One approach is to track the method instanceã€‚

 as illustrated in this diagramã€‚And the other is to determine the method based on the number and types of parameters in the invoke methodã€‚

And we have implemented both methods and note that this tracking requires global data flow analysisã€‚

The second challengeï¼Œ how do we make reflection analysis work within code L's data flowã€‚

We mentioned earlier that additional value step can define edge to connect to nodesã€‚

 But here's the problemã€‚ You can't use data flow inside additional value step because doing so would cause a non monoonic recursion issueã€‚

Basicallyï¼Œ you need data flow interface in additional value stepã€‚

 but the data flow interface depends on additional value stepã€‚ So as illustrated in the diagramã€‚

So how to fix thisï¼Œ we canï¼Œ we can create a copy of the data flow analysis implementation and have our reflection analysis rely on the copied version as illustrated hereã€‚

 And then we patch the original data flow here to avoid any dependency issuesã€‚

And to support the invoke methodï¼Œ we fix the parameter propagation logic for co in and co throughã€‚

 For exampleï¼Œ for co inï¼Œ augmented propagates to parameter and object propagates to the parameter thisã€‚

And here's an example for reflectionã€‚Firstlyï¼Œ the data flow breaks other invoke method at line 13 hereã€‚

And by adding reflection analysis as shown in steps 12 4ã€‚

 we can propagate from 10 S here to 10 object hereã€‚ And thus we get the complete pathã€‚And nextã€‚

 pass by value in Javaã€‚So firstï¼Œ what is a pass by value in programming languagesã€‚

 parameters are generally passed either by value or by referenceã€‚ And in Javaã€‚

 the way actual parameters are passed to methods is passed by valueï¼Œ not pass by referenceã€‚

 So if the parameter is a primitive typeã€‚ and what gets passed is a copy of the actual value of the primitive type and a copy is createdã€‚

 And if its a reference typeï¼Œ what gets passed is a copy of the address value in the heap of the object referenced by the actual parameterã€‚

 And just like with parametersitivesï¼Œ a copy is createdã€‚So what's the problemã€‚

 And on the right is a case where a copy is stored in a fieldã€‚

 But the problem is there are many copies like a person P and D dot A here in this exampleã€‚

 So the core issue is that when the value is modified in one copyï¼Œ all copies need to be updatedã€‚

So in this caseï¼Œ it is the pair parametermeter person P in the constructor of demo field instance hereã€‚

 the copies are indicated with theã€‚Blue line hereã€‚And here's our solutionã€‚So firstã€‚

 we have to locate the fieldï¼Œ which is of a non primitive typeã€‚ And in this exampleã€‚

 it correspond to field A for this fieldï¼Œ finance store operations identifying both non post update node and post update nodeã€‚

 So in this exampleã€‚Sorryã€‚itï¼Œ it correspond to line 7 and line 11ã€‚ line 7 is a non post update nodeã€‚

 While line 11 is a post update nodeã€‚ And for non post update nodeã€‚

 store operations use global data flow to locate the parameter and the actual argumentã€‚

 So in this exampleï¼Œ and as shown it in the red dotted line hereã€‚

 we located the parameter P here in the demo field constructorã€‚

And for another store operation of a post update nodeï¼Œ we add a mapping from this node to this nodeã€‚

So this is the jump step we have previous introducedã€‚

And we have to use global data flow because we might encounter scenarios like interprocedural callsã€‚

 store and load operations and so onã€‚And then Ill introduce the results of our researchã€‚

We have found about 190 source and sync functions across 18 goal frameworksã€‚

 We also scanned over 5000 projects and found that the detected data flows increased by more than 15%ã€‚

And we use this CE we found to show how a new sink could lead to a detectionã€‚

 And this is a sQL injection bug in traffic ops part of Apache traffic control projectã€‚

 This project has Q code Q L enabled hereã€‚ So which means it is a force negative for code Q Lã€‚ðŸ˜Šã€‚

And here's the data flow of the bugã€‚ First step is to read the parameters from user input hereã€‚

 which is a user's commentï¼Œ and then process and validate the requester parameters as illustrate here and this info dot go fileã€‚

 And finallyï¼Œ when inserting the user's comment into the database hereã€‚Hereã€‚

 it caused the query row X functionã€‚But when we checked code Q L's detection rulesã€‚

 we noticed that query Ro X function isn't included in the sync modelã€‚

And we scanned the simple X functionï¼Œ framework and found this sink as illustrated hereã€‚ And thusã€‚

 we detected the bugã€‚And with our enhancementï¼Œ we can detect historical CV Es that were previously undetectableã€‚

 And in this caseï¼Œ adding the source function enables the detection of three CV Esã€‚

 Several open source projects are affectedã€‚ðŸ˜Šï¼ŒAnd for this Cï¼ŒEã€‚

 we fixed cross thread and missing sync roleã€‚ And thus we can reproduce this CEã€‚ðŸ˜Šã€‚

And this C proves the effectiveness of a reflection enhancementã€‚And we also found many new bugsã€‚

 And here are some cases belowã€‚And finallyï¼Œ are our takeawaysã€‚

 So first is that semantic analysis of code in is particularly suitable for LO M assisted analysisã€‚

 And their combination is a research directionã€‚ Se is that code Q L's data flow analysis mechanism is highly representativeã€‚

 serving as a good start for learning data flow analysisã€‚ðŸ˜Šï¼ŒAnd thirdã€‚

 code Q L data analysis is not perfect and can be studiedï¼Œ modified and improvedã€‚So any questionsã€‚

 And alsoï¼Œ please feel free to reach us through the emails hereã€‚Okayï¼Œ thank youã€‚

