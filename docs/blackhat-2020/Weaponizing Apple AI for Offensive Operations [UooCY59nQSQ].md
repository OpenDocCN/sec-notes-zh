# Weaponizing Apple AI for Offensive Operations [UooCY59nQSQ]

Thanks for comingã€‚ So todayï¼Œ I'll be presenting a topic about weaponizing Apple AI for offensive operationsã€‚

 rightï¼Œ So I'm a lead red teamer work for CB healthã€‚

 and I'm specialized in more in post exploitations and techniques that you knowã€‚

 usually differenters don't seeã€‚Rightï¼Œ I used to write blogã€‚ and if you like my workã€‚ And alsoã€‚

 by the wayï¼Œ all my code p O that I'm gonna present today will be posted in my blogã€‚

 And if you like to take a look at itã€‚ you knowï¼Œ you can follow my blogã€‚

 if you're in team space or AI researching and stuffï¼Œ you can also follow me on LiedInã€‚Rightã€‚

So todayï¼Œ I'll be discussing more about Apple AI stack as an overviewã€‚

 like you know what all the frameworks are there and native components of Apple and the main component is core Mlã€‚

 which is a legacy frameworkã€‚ And we we see how we can weaponize that for C two operations and payload stagingã€‚

 And also there are other native frameworks called visionã€‚ andã€‚Foundationã€‚Which is now built for Macã€‚

 And we'll also see how we can weaponize that for other offensive purposesã€‚ rightã€‚

 I also built a framework called M L Rã€‚ It's a C2 frameworkï¼Œ whichã€‚

Completely whats based on core M Lã€‚ And I'll show you demo how this work and how payload staging can be performed with MR and communicates completely with native Apple APIsã€‚

Right and alsoï¼Œ nowï¼Œ in case if you don't want it to use M R Ca C2ã€‚

 we can also use mythic Apple now Apple payload and see know how we can embed that payload intoã€‚

The M model and see know how we can weaponize that for offensive purposesã€‚

 So all this will be demo as wellã€‚ and the code will be posted in my blogã€‚And alsoï¼Œ as an endã€‚

 we will also discuss about direction blind spots and know how we can mitigate this from a defensive side of itã€‚

Rightã€‚So AI stack overviewã€‚ So there are multiple components when it comes to Apple AIã€‚

 The legacy component is in core Ml engine itselfã€‚ It's a lightweight shipped with iPhone's Mac and used forã€‚

 you knowï¼Œ day to day normal activities been used by you knowï¼Œ iPhone photos and other applicationsã€‚

 And also it's been used by third party applications as wellã€‚

 And there is a newer framework called Apple Foundation modelï¼Œ which is likeã€‚ðŸ˜Šï¼ŒA large scale modelã€‚

 which is used for otherï¼Œ you know large scale activitiesã€‚

 but it is like closed private and it is used only by Appleã€‚

 So well be targeting more on the corem engine rather than the latest one because this corem engine is still available on all the latest Macã€‚

 and it can be weaponizedã€‚On top of itï¼Œ there are other frameworks that you see Viã€‚

 A V F and reality kit photosï¼Œ3ri and all those things were thereã€‚

 Those are all native frameworks of Appleã€‚ And I'll be picking up now 12 in this caseã€‚

 now the vision framework and also the A V F and see how we can weaponize thatã€‚So what is an M modelã€‚

 So M model is core M modelã€‚ So it's used by core Ml model engineã€‚

 So if you look into Apple MacBooksã€‚ you knowï¼Œ there are tons of Ml model files sitting on your desk which being used to buy different applications for different purposesã€‚

 It is a lightweight model that will be shipped along with your applicationsã€‚

 and it's designed for fast and on device executionsã€‚ and it is very lightweightã€‚

 So it can be run on mobile phonesã€‚ So whatever that we want to discussã€‚

 it is still applicable on the mobile devices as well when it comes to offensive purposesã€‚

The assumptions areï¼Œ you knowï¼Œ the model file is not signedã€‚

 So the gatekeeper or the notization does not check whether you know the model file or whatever your application is lowering is signed or it is secureã€‚

 So that's whyï¼Œ you knowï¼Œ we'll be using that for offensive purposesã€‚

So there is another format called M L model Cï¼Œ which is a compiled format of a model Ml model file itselfã€‚

 And that is alsoï¼Œ if you look into all your Mac applicationsï¼Œ most of the files you might seeã€‚

Dot Ml model or dot M model Cï¼Œ which is you knowï¼Œ it's aï¼Œ it's a compilation of model fileã€‚

 That's it all these are in a nonhu readableã€‚ So even if you try to read the model fileã€‚

 you need a core Ml framework to load and read the content of itã€‚So what's inside on a model fileã€‚

 So there are multiple components involvedã€‚ and the code components are the model descriptionsã€‚

 which have your known names typesã€‚ What this model is all about and stuffã€‚

 And the model type and the model parameterã€‚ So the model parameter is the code parameterã€‚

 which actually sets all your trained data will be sitting in your model weightã€‚

 And there are like 10 to hundreds of model layers that you can use in that particular parameterã€‚

 But you knowï¼Œ this is where all your payloads can sit as a model weightã€‚

 And I'll be showcasing how I'm gonna weapon that as a model weight and see how we canã€‚

Put a payload encrypted payload and see how we can bypass all those thingsã€‚ And there are metadataã€‚

 and I'll be demoing two things hereã€‚ One is how payload can be injected in model rates and another one is how payload can be injected in metadata of a model and see how we can load the model and execute the payloadã€‚

Rightã€‚So how M model worksã€‚ So basicallyï¼Œ you knowï¼Œ when you're building an applicationã€‚

 you wanted to use Apple AI for efficiencyã€‚ What usually the developers will do was they will use a model file Ml model file that they can build create and train the date train the model and ship it along with your applicationã€‚

 compile it into a model C file or in a model fileã€‚

 and then you can ship it with your application that that's what you know when you try to load along with that the applicationã€‚

 the model file will also get load and execute it in memoryã€‚So the next one is the vision frameworkã€‚

 It's not an Apple AI stuffï¼Œ but it can be used for sending data to Qml pipelineã€‚

 So vision framework is used forï¼Œ you know vision related activities like photo processing image processing and stuff and text recognition and it is a lightweight and very silentã€‚

 it has its own native APIs for processing all the dataã€‚

 I'll be showcasing how we can embed data into pixel on an image and how we can extract those pixel data from using that vision APIpisã€‚

The next one is the A Foundationã€‚ A Foundation is used for audio video processingã€‚

 There are many applications use AV foundations as after today like you know Zoom cap kit and IMovie there are much more applications A foundationã€‚

 It is again another native component that in Mac and used for audio video processingã€‚

 So I will be demoing on how we can create audio file and instead of putting an audio payload into a metadata or a description or somewhere elseã€‚

 we will be putting a payload directly in audio syn and then see how we can extract that using the A foundation librariesã€‚

ðŸ˜Šï¼ŒSo abusing each layersã€‚ The first one is the model fileï¼Œ like I said you knowã€‚

 we can place payloads either in the description or metadata or in the model weightsã€‚

 each date each layers have different types of formatsã€‚ So for exampleã€‚

 if you are placing your model if you payload into a model weightsï¼Œ itll be a float variableã€‚

 So if someone try to read itï¼Œ all they see is a float dataã€‚ That'sã€‚

And the descriptions has its own way of formattingã€‚ Likewiseï¼Œ you knowã€‚

 we can place your payloads on every layers of a model fileã€‚

And if why because why no one able to read this becauseï¼Œ you knowã€‚

 E D R our AV engine was unable unable to detect these kind of things becauseï¼Œ you knowã€‚

 it actually need Q libraries to read the model file and extract the dataã€‚And the vision frameworkã€‚

 So like I saidï¼Œ we should have its own set of Apisã€‚

 which is used for reading extracting data from an imageã€‚

 and we'll be using visions libraries to extract an image read an imageã€‚

 extract the pixel data and put it in a readable formatã€‚

 So each component used for different purposes and well be showcasing everything on how we can itã€‚

 So for exampleï¼Œ when I say pixelï¼Œ you create an imageã€‚ And you set the opacity like 15 personã€‚

 So when you open the image that image looks like a readable image like you know it's not a corrupted data or somethingã€‚

 you can still view the imageã€‚ But the the payload actually sits in the image as a pixel and for simplicity purposes for demo I set it to 15 person so that you know you can see what's happening what is sitting thereã€‚

 But you know you can also set it to like one personã€‚ So it's not visible to human eyeã€‚

 So if someone try to open the image by defaultï¼Œ it look like illit image and or you can inject theã€‚

Lo into a logo file of your application and ship it along with your applicationã€‚

 So during your runtime processï¼Œ you can extract the payload from the image using this native APIã€‚

 and which looks completely legitã€‚And the A Foundation is kind of something interestingã€‚

 It's not a technographyã€‚ I meanï¼Œ it's kind of stnographyã€‚

 but it's kind of a bit more advanced because what I'm doing is we will be placing payload as amplitudeã€‚

 for exampleï¼Œ I have a payload like hello world or somethingã€‚

 What I'll be doing is take the payload convert into a binary zeros and onceã€‚

 And the one represented as like1 do as an amplitude and zero is represented like 0 do2ã€‚

 So we is kind of an allowed1 do zero is kind of allowed and0 is like like modeã€‚

 So when you play that audio it still plays in we see something whatever you play itã€‚

 it still plays all you hear is a beep sound like ups and downs of beep soundsã€‚

 But you can use a foundation as a frameworkã€‚ read the audio file extract the payload and decode that and then pass it for further operationsã€‚

 So you can it's more like a technographyã€‚ but kind of bit of advanced because the audio file still look legitã€‚

 It's not a metadata or something we are actually placing the payã€‚ðŸ˜Šã€‚

L intoto as an amplitude of an audio fileã€‚Rightï¼Œ I'll show a quick demo of Vi and A Foundationã€‚

 how this thing actually worksã€‚2 soã€‚Have have a Python script that actually uses Python libraries to generate an image fileã€‚

 So when youï¼Œ when you run the codeï¼Œ it will just create a imageã€‚ And if you look into that imageã€‚

 it's like know a normal working imageã€‚ and it's not corrupted or anythingã€‚

 all you see is pixel dataï¼Œ which have a payload in itã€‚ So in this caseã€‚

 Im hard codinging encryption key for my payloadï¼Œ which I'll showcase how I'll be using thisã€‚

 but this is just not an exampleã€‚ like you know how we can place a payload or a key or something on an image as a pixel and how we can extract itã€‚

 So to extract itã€‚ you just use I wrote another script in Python which is use vision librariesã€‚



![](img/7cdacd86ecf67221c0131a9ab300fc61_1.png)

![](img/7cdacd86ecf67221c0131a9ab300fc61_2.png)

To read that image and extract that payload from the image from the pixelã€‚Rightã€‚

So this is how you can use vision to read and extract payloads during execution of your payloadsã€‚



![](img/7cdacd86ecf67221c0131a9ab300fc61_4.png)

![](img/7cdacd86ecf67221c0131a9ab300fc61_5.png)

So the next one is the A V Foundationã€‚

![](img/7cdacd86ecf67221c0131a9ab300fc61_7.png)

So this I created a two swift codeã€‚ One is to generate an audio fileã€‚

 and another one is to read the audio fileï¼Œ extract the dataã€‚ For exampleã€‚

 what the script will do is I have a code have a payload called hello worldã€‚

 And what I'm doing is I'm converting that hello world into a binary zeros and onesã€‚

 So if you look into the code So zeros are representing as amplitude of02 and one the one bit is representing as one do0ã€‚

 So when you play this audio fileï¼Œ all you see is like a beep soundã€‚

 but technically know it's not corrupted or anythingã€‚ even during the deep analysisã€‚

 it youre still unable to extract what exactly sits inside the audio fileã€‚

 But through audio foundation librariesã€‚ you can pass this fileã€‚

 So I have written on another code to read this audio file and extract the payloadã€‚

 So what this will do is it willã€‚Read the audio file and convertã€‚

 extract all the binaries and decode the binary and extract the nucle X data from that audio fileã€‚

 So the audio file look legitã€‚ And it's not detected by any of the scannersã€‚

 but you can still send literally like one number of payloads in a audio file and extract it through a engineã€‚



![](img/7cdacd86ecf67221c0131a9ab300fc61_9.png)

All rightã€‚So the next one is an Ml Arcã€‚ So M Arc is I wanted to test all this hypothesisã€‚

 like you know how this M model itself can be weaponized for payload executions and C2 operationsã€‚

 So this is lightweight C2ï¼Œ which is written and Pythonã€‚

 and it's used uses fast A for communication between client and C2ã€‚So insteadï¼Œ you knowã€‚

 Jason or D L L willll be using model file as a payload transmission mechanismã€‚So what itll do isã€‚

 So we have consider we have a C2 and an agentã€‚When you so we have a dropperã€‚

 like you know dropper is written in Pythonã€‚ So when a C2 C2 is running and you run the dropper on one of the victim system on a MacBookã€‚

 So as as you run itï¼Œ itll go and register to the to my Mr C2 and it'll register the sessionã€‚

 itll create a session for youã€‚ And then if you want to send a commandã€‚ for exampleï¼Œ knowã€‚

 I got a sessionã€‚ and I wanted to send a command like know who am I or you know something from my C2 what my C2 will do with it will take your command convert it into hex encode it or or Zar encode itã€‚

And then place it into model metadataã€‚ It'll create a model file in memory and then place the payload into model metadataã€‚

 convert that model file into binary and send the binary over the networkã€‚Rightã€‚

 so on the victim sideï¼Œ the victim received that binaryã€‚

Create recreate the model file from the binaryã€‚And use quol libraries to extract that hex encoder data and decode itã€‚

 And then execute itã€‚ And then what itll do is it will again send it back the output to the C2 again through M modelã€‚

 So again use quol libraries to recreate a model file by placing the output into the same way by vera and then send it back to the Mã€‚

 So M can use Q Ml engine to decode itï¼Œ extract the output and display it to youã€‚

 So that's how this M C2 worksã€‚So I can show a quick demo of how exactly this is workingã€‚Rightã€‚

 so I have a server which is written in Pythonã€‚ and also the agent is written in Pythonã€‚

So when you execute it the serverï¼Œ all it does is now it just listen for client communicationã€‚

 And then on the other sideï¼Œ I go to an agentã€‚ againï¼Œ it is written in Pythonã€‚ As soon as you run itã€‚

 it'll create a session for youã€‚And go to go back to your Cus serverã€‚ and you'll seeï¼Œ you knowã€‚

 there is a callback beacon from one of the agentã€‚And thenï¼Œ you knowï¼Œ to come connect to that agentã€‚

 all you do is just connect and pass the sessionï¼Œ I Dã€‚ And now you are connectedã€‚Nowã€‚

 if you want to send a commandï¼Œ likeï¼Œ for exampleï¼Œ I'll be using L S iPhone L A as I inputã€‚

 And what's happening isï¼Œ as in as you send a command to the C2ï¼Œ C 2 take this commandã€‚

Create a modelï¼Œ inject the data into model metadata and then send it to the server conent to binaryã€‚

 and the binary value will be send to the serverã€‚Andã€‚Sorryï¼Œ send it to the agentã€‚

 So the agent receives itã€‚ Decode it and run the command for you and then take this output and again output put it in a model file using Qml libraries and send the raw data binary data to the serverã€‚

 The server againï¼Œ like I saidï¼Œ itll extract that using again Q Ml engine and displayed to youã€‚

 So this is completely this M C to is completely work based on on core librariesã€‚

 and it is completelyth It's being tested against later speed and A enginesã€‚

 none of them been caughtã€‚All rightã€‚Okayï¼Œ so in case if you don't want to use M MR as a C2 because there are hundreds of C2s as available as of todayã€‚

 and you don't want to use MRï¼Œ which makes senseã€‚ So the wellknow C2 for Mac is Myic and appel is one of the wellknow payloadã€‚

 So if you run appel as a standal alone like you know appel code it'll get detected by latest Esã€‚

 So to bypass that what you can do is you can take the appel codeã€‚

 So what I'm gonna do now is I'll take the appel code and encrypt it using Zã€‚

 I will place the Z key into an image file and then use vision framework to decode that payload soã€‚

Firstï¼Œ generate a key using image and then place the key in an image fileã€‚ Nextã€‚

 take a Appleel code encrypt it using Zar and then put it on a text fileã€‚

 Now you have your encrypted code as a text file and your keyã€‚

 So now what I' will be doing is use take that encrypted fileã€‚

 use quo libraries and generate a model fileã€‚ place the encrypted code into model weights and then send it to the loaderã€‚

 rightï¼Œ So I can show you a quick demo how this thing workã€‚Soã€‚First thing you generate a keyã€‚

 This is now already we have seen like how we can generate an image using Pythonã€‚

 So this image is placed called Z keyã€‚ the same the same thing that we seen alreadyã€‚

 This is the key that were gonna use for encoding the Ael codeã€‚



![](img/7cdacd86ecf67221c0131a9ab300fc61_11.png)

![](img/7cdacd86ecf67221c0131a9ab300fc61_12.png)

Nextï¼Œ we will be creating encrypted text file like knowã€‚

 take take your appel code means Apple dot Js and take the J S fileï¼Œ use our encryptionã€‚

 use the key and encrypt the Appleel codeã€‚ So you will see a text file got createdã€‚

 which contains your know encrypted appel codeã€‚

![](img/7cdacd86ecf67221c0131a9ab300fc61_14.png)

![](img/7cdacd86ecf67221c0131a9ab300fc61_15.png)

Nowã€‚You need to embed that Appleel code into an Ml modelã€‚ So what the third step will do isã€‚

 you knowï¼Œ there is a Python Python scriptï¼Œ which usesã€‚CMl librariesã€‚

 it will take the encrypted code from the text file and place it into model weights and generate a model file for youã€‚

 So this is the dot M model fileï¼Œ which you can ship as part of your loaderã€‚

 So now what your loader will do is it will take the M model file and your image fileã€‚

 consider youre building an application and you have your model file along with your application and you send it to the victimã€‚

 So the victimï¼Œ you can place your loader like you know a post runner or no prerunner along with your application package bundle it and send it to the victimã€‚

So when you run itã€‚All you need is the model file and the image fileï¼Œ the image fileã€‚

 you can place it like a logo or somethingã€‚ right now you can completely execute this in memoryã€‚

 So what I'm doing is I have another loaderã€‚ What this is doing isã€‚ðŸ˜Šï¼ŒIt takes the model fileã€‚

 takes the image fileï¼Œ use vision to read the key and then take the model file from use core Ml to load the model file in memory and decode it completely in memoryã€‚

 extract the payload and pass it for executionã€‚Rightï¼Œ so as soon as you executedã€‚



![](img/7cdacd86ecf67221c0131a9ab300fc61_17.png)

You'll get a beacon on your Me Coã€‚So this is how you canã€‚

Run app file along now by embodying into a M L model fileã€‚ And this is completely stealthã€‚

 and it's never been detected during any of our operationsã€‚



![](img/7cdacd86ecf67221c0131a9ab300fc61_19.png)

Rightã€‚Protection blind spotsã€‚ So this model files are static filesã€‚

 which never been scanned by a engines or EDsï¼Œ because you knowï¼Œ even if you scan itã€‚

 it is really hard to find what is sitting insideã€‚ And this audio files again it's a legitimate file when you play it at still playã€‚

 So it doesn't look fishy and also the same goes for the image file as fileã€‚

 So you inject your key into an image logo or somethingã€‚ and you send it along with your packageã€‚

 The logo looks legitã€‚ And the image file never been detected as suspicious right So MR uses all this techniquesã€‚

 And that's how we were able to bypass all the E D as up todayã€‚ðŸ˜Šï¼ŒSo as a mitigationã€‚

 I would say know look for what is actually loading them a model file and where this model files from and look look for APIpis that these these applications are the loader is using like look for suspicious API know if application doesn't need to load image vision libraries and why it is calling look for those suspicious API calls which has been used by an application And that's how you can detect because you know this is not just another attack or techniqueã€‚

 this is know if you look deeperã€‚ this is like a supply chain attackã€‚

 it can also be a supply chain attackã€‚ So there are hundreds model files being posted in public forums and the developers is blindly loading those model files for the day today workã€‚

 And this also can be a supply chain attack So detect this always look what As are being called and whether it is suspicious or not apart from looking for just this is a binary or DL look you know what is doing andã€‚

Whether this image file or model files is look legt or is actually needed for that applicationã€‚

 And that's how we can detect itã€‚ As up todayï¼Œ none of the E DRs are A engines scan for model filesã€‚

 And that's whyï¼Œ you knowï¼Œ you can weaponize thisã€‚ And you guys haveã€‚

 you know write custom rules to identify these kind of attacksã€‚That is all for the talk todayã€‚

 and Im happy to answer if you guys have any questionsã€‚

