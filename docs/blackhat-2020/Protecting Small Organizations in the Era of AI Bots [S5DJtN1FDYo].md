# Protecting Small Organizations in the Era of AI Bots [S5DJtN1FDYo]

Thank you for having meã€‚ So I'm gonna to talk about protecting small organizations in the era of AI botsã€‚

 And I think a good place to start this discussion is to talk about the fact that like 51% of the Internet is now nonhuman trafficã€‚

 So this is a report by Ireva 2025ï¼Œ the bad bot reportã€‚ 51% is nonhumanã€‚

 which means that we've kind of passed a threshold where machines are now a major part of the Internetã€‚

 they are kind of taking over the Internet in a senseã€‚

 and 80% of malicious bot Is were not listed in popular I blocklistsã€‚ This is by Chiiago in 2021ã€‚

 they observe that these blocklists that are publicly availableã€‚

 a lot of the malicious Is are not not available because they're changing so rapidly and changing so frequentlyã€‚

 So I want to talk about this kind of bring this down a levelã€‚ This is kind of a global perspectiveã€‚

 B this down to kind of local organization perspectiveã€‚

 So I work with a community science Institutesituteã€‚ It's a public nonprofit promote scientificã€‚

Literacyï¼Œ volunteer water quality monitoring and certified lab analysis for Central New Yorkã€‚

 The Community Science Institute is a small organizationã€‚ They provide a few serversã€‚

 not a large not a lot of number of serversã€‚ and they provide this data for free to the publicã€‚

 So they provide this kind of public serviceã€‚ They encouraged citizen scienceã€‚

 And they provided a database lab for stream lake chemistry for harmfulgal blooms and biomodoringã€‚

 And what we observed is that a single server was receiving 150000 page hits over 20 days corresponding to over 7000 hits per dayã€‚

 So they were getting a lot of trafficã€‚ We knew that this couldn't all be attributed to humansã€‚

 of courseï¼Œ because this was kind of a lot more than anticipatedã€‚

 The traffic was so severe that was degrading server performance for Ci for known users and clientsã€‚

 So it was kind of degrading the performance of the serverã€‚

 and we clearly could could attribute it mostly to AI bos an early investigation kind of just looking at log logsã€‚

Kin of directly noticing that visitor traffic is from the entire worldã€‚

 despite the fact that the CsI database is entirely data for central New York Stateã€‚

 So even though their data is provided for central New York Stateã€‚

 we're getting traffic from all over the world up to thousands of machines touching this one server to try and gather data and gather informationã€‚

 So it's pretty easily attributable to AI crawlers to crawlers trying to gather data to train AI machinesã€‚

 What tools are availableã€‚ I'm going to very briefly mention them throttling is ineffective because actually most of these crawlers observe rate limitsã€‚

 We found that they actually observe the throd wheeling limitsã€‚ Public block lists are ineffectiveã€‚

 up to 87% are not listedï¼Œ Gr is ineffective because it's very low levelã€‚

 You're just getting single Is and doing searches like thisã€‚

 Go access W stats are's not effective because you're getting summary statistics that hide those details about whether something is a machine or non-humanã€‚

ðŸ˜Šï¼ŒReal time monitoring do not examine historic log patternsã€‚

 So we wanted to really understand what was going on by looking at logsã€‚

 And there are newer techniques that are focused on AI using AI to detect activityã€‚

 but they often depend on having really good pretraining dataã€‚

 on having really good separation between human and machineï¼Œ soã€‚

That's a challenge because you need that kind of baseline in order to train AI models to do thisã€‚

 So we're taking a non AI approach to handling an AI problemã€‚If we look atï¼Œ for exampleã€‚

 just a quick look at go accessï¼Œ you can see that it tells you by day what kind of traffic you haveã€‚

 It tells you like here's the traffic you go ball you would get on particular daysã€‚

 but it doesn't distinguishã€‚ You can see right thereã€‚ It says including spidersã€‚

 So it's telling you right there in the description of their interface that actually it's include spidersã€‚

 So it's not distinguishing between human and nonhuman usersã€‚

So one of the methods that we're gonna approach this withï¼Œ How do weã€‚

 How do I solve the problem with a clientï¼Ÿ So I the question I think that really comes down to isã€‚

 how do we distinguish human access patterns from machinesã€‚

 Can we distinguish the limit the way that people access machine people access servers versus computersã€‚



![](img/5e84d347fb22878174f9c0082cc6c31a_1.png)

And Iï¼Œ I've never been in a submarineã€‚ but I think that there's probably likeã€‚

 from I watch enough movies that there's probably a radio operator either in thereã€‚

 like listening for trafficï¼Œ likeï¼Œ okayï¼Œ is this a enemy subï¼Œ What does it sound likeã€‚

 hass that mechanical patternï¼Ÿ It's very repetitiveï¼Œ rightï¼Ÿ

 So you're listening as a radio operator in a submarine versus a whale or something naturalï¼Œ rightã€‚

 So that'sï¼Œ I think the approach that we're taking isï¼Œ does it sound like machineã€‚



![](img/5e84d347fb22878174f9c0082cc6c31a_3.png)

Does it sound like it's mechanicalï¼Œ And that's an approach that we can take to looking at trafficã€‚

I was inspired by a visualization that was done by Jung Yi Kim in a paper called Web server log Visualization in 2018ã€‚

 in which he plotted time versus host I Pã€‚ And that gives you kind of an overall perspectiveã€‚

 So the first thing we did was to visualize this for the CsI data and plot time versus host I P And the beneficial is visualization is that you get the entire log in one snapshotã€‚

ðŸ˜Šï¼ŒThe other benefit is there's no statistical summaryã€‚ There's no hiding of any dataã€‚

 You're not getting a statistical like averaging of the dataã€‚

 You're seeing the entire picture of all the log in that one imageã€‚

 and it's easy for humans to see patterns in that dataã€‚

 It's easy for us to recognize what's going onã€‚ So I ask youï¼Œ if you look at this imageã€‚

 This is kind of like the sonar guy listening to the radioï¼Œ listening to the Internetã€‚

 What do you think is human hereã€‚What do you think is humanï¼Œ rightã€‚Yesï¼Œ that's rightã€‚

 The the things that are very regular and repetitiveã€‚

 remember the time is on the horizontal axis over 20 daysã€‚

 So what's human here is the things that are not regularï¼Œ not mechanicalï¼Œ rightã€‚

 Those mechanical things are the repeated dotsï¼Œ the dash patternsï¼Œ rightã€‚

 So we can say that these things are all probably not humanã€‚

 and there's some interesting patterns hereã€‚ Of courseã€‚

 there's a lot of other stuff here that's not humanã€‚

 But these ones are particularly interesting because they have patterns that are that are still mechanicalã€‚

 but they don't follow a straight line across right So the straight line across is just your attacker who's trying to grab things just continuously throughout the day from one Iã€‚

 but these other ones are more interesting because they have they're still mechanicalã€‚

 They show these kind of sound features of being mechanicalï¼Œ but theyre they're more complexã€‚

 So we're interested in distinguishing mechanical access patternsã€‚

 regardless of whether they are benign or maliciousã€‚

And the reason for that is that we found that a lot of them are observing rate limitsï¼Œ rightã€‚

 A lot of these crawlers are observing rate limitsã€‚ so they're not acting maliciously necessarilyã€‚

 How fast are youï¼Œ no more than 20 pages per minute is like a common rate limiting approach that you would takeã€‚

 We found that most traffic was observing the rate limitsã€‚

 Most traffic was because they basically knew if you think of it promo policy perspectiveã€‚

 they knew that if they tried to access faster than thatã€‚

 then the classical throtling would come in and block them rightã€‚

 So these data centers and crawlers essentially observing rate limitsã€‚

 They're following the rate limits so that they can try to get accessã€‚

 So that's not enough to just try to do rate limitingã€‚ If you do rate limitingã€‚

 it only reduces the traffic by 33%ã€‚ and those red dots there are where rate limiting is happeningã€‚

 those are ones that are kind of still acting very aggressivelyã€‚

 much they're faster than 20 pages per minuteã€‚ and it only reduced the traffic by roughly 33%ã€‚

 And that's actually a kind of a little bit over inflated because they're actually a large number of hitsã€‚

Happening from thoseï¼Œ rightï¼Œ you can see that all that other traffic is still happeningã€‚

So what other patterns that humans would followï¼Œ That's throtlingã€‚ How fast are youï¼Œ rightã€‚

 We saw that one alreadyã€‚Con how often do you visit is an interesting oneã€‚

 So this is like this is starting to get into human computer interaction and borrowing some ideas from human commit interactionã€‚

 That field studies a relationships between computers and humansã€‚

 And I think there's a lot of interesting overlaps between that field and securityã€‚

 human computer action saysï¼Œ how do humans use machinesã€‚ How do people use machinesã€‚

 And if you think that machines are basically the inverse of thatã€‚ So we don't operate very fastã€‚

 we we are slower at navigating pagesã€‚ But there's other thingsã€‚

 We don't access a machine for more than five days in a rowï¼Œ perhapsã€‚

 or especially if you combine that with daily rangesã€‚ That isã€‚

 most people can't work more than six hours a dayã€‚ And if you're browsing one page for three or four hours a dayã€‚

 that's pretty unusualï¼Œ rightï¼Ÿ And especially if you're doing that consecutively over multiple daysã€‚

 So you combine these human factors and say these human factors actually help us to understand which things are machinesã€‚

ðŸ˜Šï¼ŒDaily hitsã€‚ How many pages do you look at in a single dayã€‚

 If you're looking at more than 400 pages a dayï¼Œ youre at our single serverã€‚

 you're probably not human because that's just really a lot of trafficã€‚

So this brings in behavioral science and human computer action into the equation and says there's this interesting relationship between HCI and securityã€‚

So we developed a kind of metric based on thisï¼Œ and that metric uses human behavioral metrics to develop a scoring algorithm based on IP hashing of the key value map to raw pagesã€‚

 we sort the pages by day and timeï¼Œ so we sort those pages because we want to look at things over a daily rangeã€‚

 humans operate on a daily basis so it's kind of useful to look at days and then we apply these behavioral metrics to the IP numbers on a daily rangeã€‚

 The scoring is based on a weighted contribution of these different metricsã€‚

So here's some intermediate resultsã€‚ Here's the original trafficã€‚

Here' is blocked by consecutive days with daily rangesã€‚ and you can see that the blocking is reallyã€‚

 really pretty good at getting those kind of longï¼Œ continuous things that are accessing the server over a continuous period of the dayã€‚

 a large daily rangeã€‚Even things that are kind of like dotted repetitiveã€‚

 It's still getting those things like those that one in the middleã€‚

 that's like a dashed line is like smart enough to get different time periods the dayã€‚

 But then doing it for a significant kind of like focused timeã€‚

This is blocking daily range with frequencyã€‚ So you have to beã€‚

 this is kind of what we call smart smart throtlingï¼Œ basically like pages per minuteã€‚

 but you also have to be over an average number of pagesã€‚And we block daily maximumã€‚

 There's very few that are trying to do this because they're kind of veryã€‚

 there's very specific and trying to get lots of traffic very quicklyã€‚

 And then commudo filtered resultsã€‚ This isï¼Œ this is the conclusion of all of those things combined togetherã€‚

 And you compare thisã€‚ This is the original trafficã€‚

And this is the filtered community filtered resultsã€‚ You see that it's a much better pictureã€‚ rightã€‚

 If you think of it it as a humanï¼Œ like analyzing this visuallyã€‚

 we've probably gotten rid a lot of mechanical access patterns hereã€‚

 But there's still some things happening here at the bottomï¼Œ rightï¼Œ It didn't get rid of thisã€‚

 It didn't get rid of these dashed lines hereã€‚So how do we clean that upã€‚Soï¼Œ what we noticeã€‚

Is I'm gonna to take you zoom in on a little picture of hereã€‚ Rememberã€‚

 this vertical is the total Ip rangeã€‚ So you're looking at a 4 billionã€‚

 The vertical axis is 4 billionï¼Œ rightï¼Œ The entire I rangeã€‚ So it's very dense verticallyï¼Œ rightã€‚

 horizontallyï¼Œ it's dense as well because you have seconds down to the second across 20 daysã€‚

 verticallyï¼Œ it's very denseã€‚ But let's take a close up of a section hereã€‚

 If we zoom in on this sectionã€‚ What we see is kind of these very each one of those little dots is a hitã€‚

ðŸ˜Šï¼ŒAnd if you look at just this part hereï¼Œ these two different linesã€‚

 they're actually distinct in the way that they appear close upã€‚ This is a single Iã€‚

 It's being hit multiple timesã€‚ It's from one I numberã€‚ Rememberï¼Œ vertical access is I numberã€‚

 And this is what multiple Is look likeã€‚ it's scattered across a lineã€‚ Nowã€‚

 if you zoom out and look at the full pictureã€‚ it's hard to distinguish thatã€‚

 because it's just a data centerã€‚ But this is a data center attackã€‚

 because it's happening across many Is within a small subnetï¼Œ a class C subnetã€‚

 right And so this is what a data center attack look like looks like in this visualizationã€‚

We can if you look at the raw dataï¼Œ it's very easy to see that because you basically see this group of IP numbers like 00ã€‚

1ã€‚23ã€‚ that whole section has very consistent traffic statistics that is are hitting a certain number of pages at a circuit frequencyã€‚

 And so this is clearly a data center attackï¼Œ basically trying to gather data from this siteã€‚

 So it's pretty easy to identify once you know what to look forã€‚

 you look for a certain frequency across a subnetã€‚The question was how to automate this a bit and how to make this more practicalã€‚

 And so what we realized we could do is basically collapse down all of the I Ps in a subnet and then do our scoring on thatã€‚

 So what we do is take those I P numbersã€‚ We collapse them down to a single to a single subnet and aggregate and score that subnet separately from the individual I Psã€‚

This is the entire algorithm just in brief for those that are technicalã€‚

 The entire algorithm basically takes in the access logã€‚

 It performs structured hits to get the accessã€‚ the log data into a structured format performs hierarchicalical I hashingã€‚

 So we I hash the first stage and do metrics on that to get the I level blockingã€‚

 Then we do an hierarchical version of that where we mask off the lowest byteã€‚

 And that gives us the masking at the next levelã€‚ We then score that level and then go to the last level class B subnets and also do scoring at that levelã€‚

 So it's the process of basically aggregating by subnetã€‚

 and then doing that hierarchically and aggregating at higher level subnetsã€‚Then finallyã€‚

 we take the total commmative resultï¼Œ and we output a number of output filesï¼Œ helpfulã€‚

 useful products that are used for thatï¼Œ that purposeã€‚ So the final resultã€‚ðŸ˜Šï¼ŒIs basically thisã€‚

 So the filtered resultï¼Œ prior to subnet hashingã€‚This is a filtered result prior to doing subnet hashingã€‚

 And this is blocking class C subnetsã€‚ So these are subnets that are within within a 255 range And this is blocking class B subnets that also show patternsã€‚

 So rememberï¼Œ you do scoring on the subnetsã€‚ It's not just sayingï¼Œ okayï¼Œ we block subnetsï¼Œ rightã€‚

 You do scoring on those subnets similar to the way we do scoring on individual Iã€‚

And the result is thisã€‚ This is the final resultã€‚And you see the difference between thisã€‚

 the original traffic on the left and the final result on the rightã€‚

RightThis is the goal was to try to distinguish human access patterns from machine access patternsã€‚

 And we got most of the way there with the behavioral scoring and then the subnet hashingã€‚

 hiarchco hashing guesses the rest of the way there because we are basically able to block those subnets that are pursuing that mechanical access patternsã€‚

 So this is a reallyï¼Œ I think a big improvement from what we had beforeã€‚

 And it shows that we can actually kind of like just using behavioral scoring techniques extract out what is human versus nonhuman to a much greater degree than just throttling alone can achieve some of you are probably noticing this vertical blip thereã€‚

 And I'm going to get back to that you guys who are really hackers in this community can probably guess what that vertical blip is thereã€‚

 I'm going to come back to thatã€‚So that we also do estimated load analysisã€‚

 And this is just using a very simple kind of fixed rateã€‚

 you assume an average page hit like response timeã€‚

 like maybe it takes 20 seconds or 60 seconds for a server to respond to somethingã€‚

 So we assume that kind of some averageã€‚ And that allows us to create an estimate of log sorry bit of load analysis over that time periodã€‚

 So the original traffic is the gray stuff you see in the background thereã€‚

 That's the original load trafficã€‚ Class C filtering takes out those things that are purpleã€‚

 So things that are purple or kind of lots lots of class C traffic happening at that timeã€‚

 Class B is the blue stuff is also being taken outã€‚

 And the final filtered server load is the bright green So we're going from what's in the background to the server load being what's in the foregroundã€‚

 that green that bright greenã€‚We observed a 94% reduction in traffic using this techniqueã€‚

 which is significantly greater than throttling aloneã€‚

 So with throtling and those estimates of the workloads are there averagever request per minuteã€‚

 you get a stage reduction of 33%ã€‚ if you add the consecutive filteringï¼Œ you get another 9%ã€‚

 if you add daily rangesï¼Œ you get another 9%ã€‚ they daily maximum ads another 3%ã€‚

 And then when you do C net blockingï¼Œ you get another 14% and B subnet blocking gives you another 26%ã€‚

 So you get a total 94% trafficã€‚ this corresponds very well to what we estimated for the community science Institute in terms of how much AI generated traffic there was And if you look at the resultsã€‚

 you'll see that that is kind of corresponds pretty well to what we see right this looks much more like now human access patterns on the right than what we had on the leftã€‚



![](img/5e84d347fb22878174f9c0082cc6c31a_5.png)

![](img/5e84d347fb22878174f9c0082cc6c31a_6.png)

So we found that even when well behavehaved and observing rate limitsã€‚

 the sheer volume of AI bot requests can overwhelm suburbs for small organizationsã€‚

 This is why we didnt we don't really look at all at request at the request at allã€‚

 right So there is there an SQL injectionï¼Œ Is there an attackerï¼Œ We just don't look at that at allã€‚

 because the problem is really on a different levelã€‚

 it's observing the fact that now there's so much traffic that even benign well- behavehaved crawlersã€‚

Cras that are just observing rate limitsï¼Œ not doing injectionsï¼Œ not trying to attackã€‚

 but just trying to gather data in a reasonable wayï¼Œ but using data centers to do itã€‚

There's so much of that volume of traffic on the Internet now that it can overwhelm small servers and small organizationsã€‚

 So if you're big organizationï¼Œ it's not a problem because you have lots of I Ps and a technical staff to deal with thisã€‚

 But if you're a small organizationï¼Œ you have just a few serversã€‚

 then you need a way to just deal with that volume of trafficï¼Œ regardless of where it's coming fromã€‚

So the policy is of the Community Science Instituteã€‚

 I want to get to policy a little bit because it does relate very much to thisã€‚

 The water quality data is available for free to the publicï¼Œ at community Science Instituteã€‚

 the client that I work withã€‚We they say that we prefer to have a human in the loop and discourage AI crawlers so that our servers remain responsive to human usersã€‚

 So it's not about data protectionã€‚ Like this organization that we work with is not trying to block the traffic because they're trying to protect their dataã€‚

 Their data is free and even provide like a free downloads page where you can literally download in bulk the entirety of the dataã€‚

 If you're a human and you did thatï¼Œ and you would just get the data right awayã€‚ But insteadã€‚

 these AI crawlers are trying to hit every single page and trying to grab all these different pages when reallyã€‚

 if it was just one human that contacted us and said we'd like your dataã€‚

 you would just get it right away for freeï¼Œ rightï¼Œ So the purpose is to actually keep our servers responsive to human users and to allow the human users to really use the servers well and just discourage that kind of likeã€‚

Blind grabbing of data by AI crawlers and data centersã€‚

Another reason for looking at this from a policy perspective for small organizations is that grants for nonprofits and small orgs are often depend on the viewership statisticsã€‚

 statistics for newer renewed fundingï¼Œ right for small organizationsã€‚

ll often ask like what are your statistics for viewershipã€‚

 And if you were to say those those original numbersã€‚

 you would say you're getting 12000 hits per dayã€‚ That's a nice numberã€‚

 but it's not accurate in any wayã€‚ because 90% of that is due to AI botsã€‚ So we can with this toolã€‚

 we can get an upper bound on human useã€‚ And I think there's still probably a lot of AI traffic in there as wellã€‚

 rightï¼Œ But at least gives a more realistic upper boundã€‚

 a more realistic upper bound of around 500 pages per dayï¼Œ500 hits per day for a single serverã€‚

 So that's a 95% reduction in trafficï¼Œ which is very different than just basic throttlingã€‚ðŸ˜Šã€‚

So the conclusions are understanding the extent of AI and crawler bot activityã€‚

Defending small organizationsã€‚ So we want to understand the extent of that activity and try to understand likeã€‚

 what are AI crawlers and bots and how do they behave that is humanï¼Œ you knowï¼Œ versus machineã€‚

 And the goal is to defend small organizationsã€‚ That is single machines from large organizations that have many machines and data centersã€‚

 And that prevents presents an interesting challenge becauseã€‚

You want to be able to find the tools cheaply and easilyï¼Œ easy to useã€‚ And alsoï¼Œ you knowã€‚

 you don't have a huge I T group that's going to solve these thingsã€‚ So how do you solve itã€‚

 So you want to be able to able to specify defense policyã€‚ How or how strongly do you want to defendã€‚

 No the extent possible the implications of those policiesã€‚ And do all of this easilyã€‚

 cheaply and open sourceã€‚ And so that's kind of where we focused our efforts onã€‚ Iã€‚

 this is being done by Q sciencesï¼Œ which is a knowledge AI and data visualization startupã€‚

We developed loggriip as a tool to do thisï¼Œ and so I'll go into a little bit what that tool doesã€‚

 It's a simple lightweight open source tool for generating block lists and policy visualizations based on access logsã€‚

So that new tool is log gripã€‚ And here's an example of it runningã€‚

 It's now we just made it available this week on Githubã€‚ you can download itã€‚ It's open source toolã€‚

 Apache2 licenseã€‚ so free to try it outã€‚ and it runs entirely from the command line and add outputs visual products from the command line as wellã€‚

 so you don't need a graphical interfaceã€‚ You can just run itã€‚

 you can batch it if you need to It only takes two inputã€‚

 the input is the access log that you're looking atã€‚

 So something generated from like journal controlã€‚ and it config file that tells that the log format and the policy you'd like to applyã€‚

 I'll show some examples of this in a minuteã€‚ and the outputs are all of these different products that you get output it at onceã€‚

 It gives you all of these things at one timeã€‚ It's fastï¼Œ it can do 150 k log in less than a minuteã€‚

 so you can handle large logs and put them into the tool and get out all of these different output products that you want to take a look atã€‚

These are all the productsã€‚ They' are all the things that I kind of mentioned throughout the talkã€‚

 It outputs the observed traffic as a P G fileã€‚ So the graphical visualizations are just done as a straight Pngã€‚

 you can just look at the imagesã€‚ It also outputs a colored oneã€‚

 which shows you the blocking actions that were takenã€‚

 it was able to filter And then it shows you the filtered traffic which is a result of that on the bottom leftã€‚

 it shows you the difference between the original and filteredã€‚

 so you can very easily as a human look at it and say here's the traffic that we started withã€‚

 Here's the traffic that we ended up with if we applied this filterã€‚

 It outputs the metrics by I as a CV fileã€‚ So you can load that into your favorite spreadsheet and then just look at what's happening with individual Isã€‚

 and basically the statisticstik for thatã€‚ And the metrics by v sububnet and the page hits as the kind of total dataã€‚

 What pages were being hitã€‚ There's a couple more products like courseet outputs the blacklist as wellã€‚

 And you can use that blacklist and drop it into to I tables or some other filter that you'd like to use toã€‚

ðŸ˜Šï¼ŒDo the actual real time blockingã€‚So limitations of the toolï¼Œ I just want to mention these brieflyã€‚

 some limitationsï¼Œ you probably guessed if you're a technical person that that vertical thing that spreads across many Is is a distributed dental service attackã€‚

And that is more a proper hacking approach because basically what they're doing is grabbing random Is across the entire vertical spectrum of Is and applying them all at a single short duration timeã€‚

 And so this is what a DD attack looks like in this visualizationã€‚

 It looks like this very short thing happening across the entire spectrumã€‚

 And you can't really approach it from I blocking because they're randomized each timeã€‚

 So the next time they might be on a different a different range across the spectrumã€‚

 So this tool is not really meant to service thatã€‚ So if you want to do blocking a DD attacksã€‚

 you kind use a different tool because this is blocklistã€‚ So that's one limitation of the toolã€‚

 another limitation is that many Acros are still presentã€‚

 but they're well describedised and more randomã€‚ So if we look at this thing on the rightã€‚

 it's random and infrequentï¼Œ it's maybe across many Is because you can see they're kind of slightly distributed thereã€‚

 It is random and infrequentã€‚ And it's below the limits of the policy that was setã€‚

 That's probably still a machine because you can see how regular it isã€‚ And itã€‚

Happens somewhat late in the eveningã€‚ You can adjust your policy to maybe grab thatã€‚

 But at this pointï¼Œ it's starting to look very much like a human in a wayï¼Œ rightã€‚

 It's behaving more like a human in the sense that it's not that oftenã€‚

 It's maybe two or five times a dayï¼Œ10 times a dayï¼Œ maybeã€‚ and it's very slow and somewhat randomã€‚

We would basically sayï¼Œ I meanï¼Œ looking at thisï¼Œ I would sayï¼Œ wellã€‚

 maybe you don't block that because it's acting more like a humanã€‚ It's slowã€‚ It's methodicalã€‚

 It's taking its timeã€‚ So maybe that's okayã€‚ Maybe it's okay to let that machine have that dataã€‚

 It's acting not in a way that's aggressively bogging down the serversã€‚

 So you can see that there's probably still a lot of traffic like thatã€‚

 It's acting less aggressivelyã€‚ That's you knowï¼Œ grabbing data more like a human wouldã€‚

 And that's maybe desirable or what we're trying to achieveã€‚

 So at some point you have to say it's close enoughã€‚

And the human and machine gets kind of harder to distinguish at that pointï¼Œ rightã€‚

 Whoever designed that serverï¼Œ Whoever designed that policy for that AI crawler did a good jobã€‚

 because they saidï¼Œ let's just make it literally look a lot like a humanã€‚

 So it's really interesting approach from the attack perspective of likeã€‚

 how do we design it so that it really behaves like its machineã€‚ I meanï¼Œ like a humanã€‚Future goalsã€‚

So briefly mention some future goalsã€‚ It's now in useã€‚ It' it's now in use by the CSIã€‚

 So we're using it to actively blockã€‚ and just very recentlyã€‚

 So in the past month we started using itã€‚ So one of our goals is to measure the post- blocklocking activity with the client and understand the realtime implicationsã€‚

 So we spend a lot of time doing the log analysis for several different month periods 20 days at a timeã€‚

 and developing block lists and now those lists are in place and we want to understand how it's actually affecting the server and what's happening with the usage of itã€‚

 another thing that's really interesting and challenging is the difference between ground truth data for humans and non-human activity both are very difficult to replicateã€‚

 If you think about replicating a humanï¼Œ you have to really kind of act like a human and be really kind of random and it depends on sleep and time of the day depends a lot of human factorsã€‚

 circadian rhythmsã€‚ So that can be difficult to replicate and also people are very specific about what they're grabbing rightã€‚

They're grabbing certain pages to try to understand somethingï¼Œ to gain knowledgeã€‚

That's hard to replicateã€‚ The other thing that's hard to replicate is the nonhuman activityã€‚

 And that's the machine that is basicallyï¼Œ you knowã€‚

 there's many different subtle ways to approach thatã€‚ The very obvious one isï¼Œ you knowã€‚

 just rapid access to everythingã€‚ But there's a spectrum in there where there's a boundaryï¼Œ rightã€‚

 And so getting the two different data sets is very challenging because there's overlap rate in the middleã€‚

And it would be niceã€‚ So we did not have ground truth data with this because this was a live serverã€‚

 And so we collected data from this and did this analysis visuallyã€‚

 but it would be really nice to have ground truth data for this going forward in the future and doing kind developing honeypots or some way to kind of gather that ground truth dataã€‚

 the other thing is to study policy parameter sensitivity and or optimize for thatã€‚

 So now has like 12 parameters that you can adjustã€‚ And that gives you quite a big parameters rangeã€‚

 And so I've experimented a little bit with itã€‚ like if you adjust the parameters up and down how how much more strongly does it block certain patternsã€‚

 and it would be nice to optimize for thatã€‚ But that optimize depends on there being ground truth dataã€‚

 So being able to optimize for parameters for thatã€‚

 say you want to kind of take a machine learning approachã€‚

 you really need some ground truth data to be able to sayï¼Œ yesã€‚

 that's a correct block or not correct block in order to optimize the policy parametersã€‚

 So there's interesting future goalsã€‚ðŸ˜Šï¼ŒI want to show a very quick video of how easy this is to set upã€‚

 Soï¼Œ so for some of you the are technical in the audienceï¼Œ I have a video of setting this upã€‚

 It's only two minutesã€‚ So you can do full setup and running of this in two minutesã€‚

 That includes building itã€‚ So I'm gonna show you building itï¼Œ tooã€‚



![](img/5e84d347fb22878174f9c0082cc6c31a_8.png)

![](img/5e84d347fb22878174f9c0082cc6c31a_9.png)

So here it's cloning the repositoryã€‚ There's two dependenciesï¼Œ Libminã€‚

 both are on our Github repository called Qantasaiã€‚Liibmin and loggri are both the two dependenciesã€‚

 So we've downloaded themï¼Œ cloned themï¼Œ doing build on Libminã€‚takes just a few secondsã€‚

 It's a pretty small kind of lightweight projectã€‚We get to watch a bill during a talkã€‚

Now we're going to build log gripã€‚Soï¼Œ that's doneã€‚Nowï¼Œ we're going to need some inputsï¼Œ rightã€‚

 So we look at assetsï¼Œ which comes from a repositoryã€‚ There's an example logã€‚

 and we'll take a look at an example logã€‚So this example log is a ruby on rails log actuallyã€‚

 and it's basically your journal control rubbyion railils log for a website for a serverã€‚

 statistics the hits are all there for get queries with all the traffic dataã€‚

 your typical log and we also need a configure policyï¼Œ the config filesï¼Œ the other inputã€‚

 So there's examples here for rubyion railils and for Apache2 and you can see that the first thing in the log is the format stringã€‚

 which is a dynamic parserï¼Œ we build a dynamic parser that basically does this regular expression parsingã€‚

 So you can put in kind of whatever log format you wantã€‚

 there's capture groups that get the different information from the log about the statistics you're looking for like the pageã€‚

 the I numberï¼Œ the date and time is really all our tool and needs because we're not analyzing the request in any more depth than that and then you can see there's also the policy settings are also in config file So the policy settings for the daily limitsã€‚

 the daily rangeã€‚Concutive rangeï¼Œ like what how muchï¼ŸWhat is a consecutive range that's in minutesã€‚

 So 2ï¼Œ40 is like 4 hoursï¼Œ roughlyï¼Œ I thinkã€‚ So it's telling it the policy limits based on the discussion earlierã€‚

Other visualization things like how do you want to do the visualizationï¼Ÿ

And then we continue from thereã€‚So now we run itã€‚ we're going to find make sure that it was builtã€‚

 So log grip is thereã€‚Nowï¼Œ we just run log gripï¼Œ and we provided those two inputsã€‚ We input the logã€‚

 and we input the config fileã€‚And now it's actually runningã€‚ It gives you some good feedbackã€‚

 It tells you that it's loaded the settings from your config fileã€‚ It's got your config fileã€‚ Okã€‚

 It's now reading the logsã€‚H00% redï¼Œ and then it does all that stuff very quicklyã€‚

 This is a very small exampleã€‚ So it ran extremely fastã€‚Writing processing Isã€‚

 building that hierarchical patch that I talked aboutã€‚

 constructing the B subnets and the C subnets from your log dataã€‚

 writing out the Is for those different subnetsï¼Œ writing out the pages and the hits and giving all those output products that I talk aboutã€‚

It shows you kind of a total summary there that you basically on each dayã€‚

 it gives you a summary and the reduction for that day based on the policy that you've setã€‚And nowã€‚

 if we take a look at the output products and output all these out files and figure1 P And G is your kind of like pre filteredã€‚

 Here's the traffic that you haveã€‚

![](img/5e84d347fb22878174f9c0082cc6c31a_11.png)

The blocking activity was the colored oneã€‚ And then this is the post filtered resultsã€‚

 So these PGs are output by the toolã€‚It also outputs the load visualizationã€‚



![](img/5e84d347fb22878174f9c0082cc6c31a_13.png)

And we'll just take a look at a couple of our outputsï¼Œ the Isã€‚

 This is a CSV file you can load to in spreadsheet that shows you all the Is and the statistics for each of themã€‚

 So there's a header at the top of this that tells you like the frequencyã€‚

 the durations and the page limits for each of the Is and an example of one of the pages that it tried to hitã€‚

And finallyï¼Œ the last thing isã€‚The block listï¼Œ which is what you wantã€‚

 this is the block list that you generatedï¼Œ it's hierarchicalã€‚

 so it's as short as possible right it does from the top down and says let's block the B nets and the C nets and then if anything that was on a specific IP is included in thoseã€‚

 it avoids including that because it's already covered by the higher level so it's as minimal as possible block listã€‚



![](img/5e84d347fb22878174f9c0082cc6c31a_15.png)

Uã€‚

![](img/5e84d347fb22878174f9c0082cc6c31a_17.png)

That's it for thatã€‚Go back to the slidesã€‚So I think I have seven minutes leftã€‚

 I think we'll use that for questionsã€‚

![](img/5e84d347fb22878174f9c0082cc6c31a_19.png)

This is how you can get and talk to me or find out more information Quaantas sciences is now on GiHub at Qantasai loggriip is there as well so you can download that and give it a tryã€‚

 there's just been an archive paper this week that we posted that talks about this and a lot more technical detail and gives you kind of a more historical and kind of review of prior work in relation to what we're doing and then if you want to find out more about meã€‚

 you can find me by my website and I also hang out a lot on LinkedIn so you can find me there too okayã€‚

All rightï¼Œ so thank you very muchã€‚And we'll do questionsã€‚

Realizing the SMBs and the variety of different informationsï¼Œ websiteï¼Œ salesã€‚

 informational utility that are out there that could typically be geofd especially for certain times a day who's going to be looking at their water data at three in the morning looking at the regularity of that would it be possible to include geofencing compared to time of day Yeah which would be another unique identifier or somebody from some weird IP address accessing consistently go Yeahã€‚

 I think it would I think it makes sense to include geoI So one of the things that you know in our future goals is to be able to do it a little bit more fine- grained and so to add more like looking at Is and doing geofencing and having a database that can say oh these Is are from a certain area or region and blocking by that So yeahã€‚

 definitely we'd want to be doing that the geofencing information is already openly available and yesã€‚

Yesï¼Œ yeahï¼Œ' that's one thing we'd like to doã€‚ The other thing we'd like to doï¼Œ tooã€‚

 is to start looking at the request in a little bit more detail because I mentioned our goal was to like do this purely at a statistical perspective for nowã€‚

 but I think it would be nice to also start investigating the request a little bit more detailã€‚

 And just using that information to say this is definitely a machine or non-machine that gets you into the whole question of you is this an attacker or notã€‚

 but this was a good starting point for usï¼Œ but we realized thatï¼Œ yesã€‚

 we can also go in and get more information from lots of different sources like such as publicly available listsã€‚

 not so much block listï¼Œ but things like geofencing are a good exampleã€‚Okayï¼Œ any other questionsï¼Ÿ

Okayï¼Œ thank you very much for comingã€‚Andã€‚So how frequently do you see yourself happen to do a scan like this for a particular small business in order to set up or refresh the block listï¼Ÿ

That's a good questionã€‚ So thisï¼Œ we're still very early in doing thisã€‚

 So it's not really known yet how often we need to do itã€‚ But what's nice about doing itã€‚

 if if you set it up well with your I tablesï¼Œ you can actually continue to grab traffic that would be incoming evenã€‚

Like before it's blockedï¼Œ rightï¼Œ So you still do the blockingï¼Œ but continue to grab the I trafficã€‚

 And we'd like to keep collecting it continuously and then see how often we need it for refreshã€‚

 I don't have the answer yet to that yet because we've been doing this prettyï¼Œ pretty recentlyã€‚

 But once we get enoughï¼Œ then we'll have a better sense of likeã€‚

 how often we need to refresh the block list yeahã€‚ðŸ˜Šï¼ŒOkayã€‚Youã€‚

